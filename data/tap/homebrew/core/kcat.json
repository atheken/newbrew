{
  "name": "kcat",
  "full_name": "kcat",
  "tap": "homebrew/core",
  "oldname": "kafkacat",
  "oldnames": [
    "kafkacat"
  ],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "Generic command-line non-JVM Apache Kafka producer and consumer",
  "license": "BSD-2-Clause",
  "homepage": "https://github.com/edenhill/kcat",
  "versions": {
    "stable": "1.7.0",
    "head": "HEAD",
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://github.com/edenhill/kcat.git",
      "tag": "1.7.0",
      "revision": "f2236ae5d985b9f31631b076df24ca6c33542e61",
      "checksum": null
    },
    "head": {
      "url": "https://github.com/edenhill/kcat.git",
      "branch": "master"
    }
  },
  "revision": 0,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "arm64_ventura": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:91a40bc28ce360a9fa3d40f9a16917cc91e553f13b8cc6bcecbff98827c9d15d",
          "sha256": "91a40bc28ce360a9fa3d40f9a16917cc91e553f13b8cc6bcecbff98827c9d15d"
        },
        "arm64_monterey": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:fd220a7e002772622e581f636e59c4a198ec883cbb813d2b31857d0bf24d089d",
          "sha256": "fd220a7e002772622e581f636e59c4a198ec883cbb813d2b31857d0bf24d089d"
        },
        "arm64_big_sur": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:f930080248bb0eff245599536bbc12465c6bf6e256acb283e6d2d5a5d047f11e",
          "sha256": "f930080248bb0eff245599536bbc12465c6bf6e256acb283e6d2d5a5d047f11e"
        },
        "ventura": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:7f2f0da947ecd7fd93bea3a0b53d4e2f0e2d51e5bd9f2b93b870c20c22f88e26",
          "sha256": "7f2f0da947ecd7fd93bea3a0b53d4e2f0e2d51e5bd9f2b93b870c20c22f88e26"
        },
        "monterey": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:5629c17a2d26e8e36c1e7aa8e54430e004e2d7ef080df3ce6ff6edee4a4eb0e4",
          "sha256": "5629c17a2d26e8e36c1e7aa8e54430e004e2d7ef080df3ce6ff6edee4a4eb0e4"
        },
        "big_sur": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:c6d947fa5cbdd948ab09082f1c961d21e5a6e565c36cb6ffffdc9712456cafad",
          "sha256": "c6d947fa5cbdd948ab09082f1c961d21e5a6e565c36cb6ffffdc9712456cafad"
        },
        "catalina": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:7125450d67cedf6116bc7e2afb22be2b860a715dc1de6663e4e75936d7eb466e",
          "sha256": "7125450d67cedf6116bc7e2afb22be2b860a715dc1de6663e4e75936d7eb466e"
        },
        "mojave": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:f403c10fd0970ed617e43cbf6fa107cac70cba94633e04c47586a586505b7ec6",
          "sha256": "f403c10fd0970ed617e43cbf6fa107cac70cba94633e04c47586a586505b7ec6"
        },
        "x86_64_linux": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kcat/blobs/sha256:954186ec95e9963cafa13f2ec0ac95591adc1a6ab1b82df116468c973c1ba51f",
          "sha256": "954186ec95e9963cafa13f2ec0ac95591adc1a6ab1b82df116468c973c1ba51f"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [],
  "dependencies": [
    "avro-c",
    "librdkafka",
    "libserdes",
    "yajl"
  ],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": null,
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/kcat.rb",
  "ruby_source_checksum": {
    "sha256": "9b9a11ec9170e08d030266b72a0b1cb358ef2a596873b17c534f366c007fc28f"
  },
  "date_added": "2021-08-29T02:22:28+00:00",
  "readme": "![logo by @dtrapezoid](./resources/kcat_small.png)\n\n# kcat\n\n**kcat is the project formerly known as as kafkacat**\n\nkcat and kafkacat are Copyright (c) 2014-2021 Magnus Edenhill\n\n[https://github.com/edenhill/kcat](https://github.com/edenhill/kcat)\n\n*kcat logo by [@dtrapezoid](https://twitter.com/dtrapezoid)*\n\n\n## What is kcat\n\n**kcat** is a generic non-JVM producer and consumer for Apache Kafka >=0.8,\nthink of it as a netcat for Kafka.\n\nIn **producer** mode kcat reads messages from stdin, delimited with a\nconfigurable delimiter (-D, defaults to newline), and produces them to the\nprovided Kafka cluster (-b), topic (-t) and partition (-p).\n\nIn **consumer** mode kcat reads messages from a topic and partition and\nprints them to stdout using the configured message delimiter.\n\nThere's also support for the Kafka >=0.9 high-level balanced consumer, use\nthe `-G <group>` switch and provide a list of topics to join the group.\n\nkcat also features a Metadata list (-L) mode to display the current\nstate of the Kafka cluster and its topics and partitions.\n\nSupports Avro message deserialization using the Confluent Schema-Registry,\nand generic primitive deserializers (see examples below).\n\nkcat is fast and lightweight; statically linked it is no more than 150Kb.\n\n## What happened to kafkacat?\n\n**kcat is kafkacat**. The kafkacat project was renamed to kcat in August 2021\nto adhere to the Apache Software Foundation's (ASF) trademark policies.\nApart from the name, nothing else was changed.\n\n\n## Try it out with docker\n\n```bash\n# List brokers and topics in cluster\n$ docker run -it --network=host edenhill/kcat:1.7.1 -b YOUR_BROKER -L\n```\n\nSee [Examples](#examples) for usage options, and [Running in Docker](#running-in-docker) for more information on how to properly run docker-based clients with Kafka.\n\n\n## Install\n\n### On recent enough Debian systems:\n\n````\napt-get install kafkacat\n````\n\nOn recent openSUSE systems:\n\n```\nzypper addrepo https://download.opensuse.org/repositories/network:utilities/openSUSE_Factory/network:utilities.repo\nzypper refresh\nzypper install kafkacat\n```\n(see [this page](https://software.opensuse.org/download/package?package=kafkacat&project=network%3Autilities) for instructions to install with openSUSE LEAP)\n\n### On Mac OS X with homebrew installed:\n\n````\nbrew install kcat\n````\n\n### On Fedora\n\n```\n# dnf copr enable bvn13/kcat\n# dnf update\n# dnf install kafkacat\n```\n\nSee [this blog](https://rmoff.net/2020/04/20/how-to-install-kafkacat-on-fedora/) for how to build from sources and install kafkacat/kcat on recent Fedora systems.\n\n\n### Otherwise follow directions below\n\n\n## Requirements\n\n * librdkafka - https://github.com/edenhill/librdkafka\n * libyajl (for JSON support, optional)\n * libavro-c and libserdes (for Avro support, optional. See https://github.com/confluentinc/libserdes)\n\nOn Ubuntu or Debian: `sudo apt-get install librdkafka-dev libyajl-dev`\n\n## Build\n\n    ./configure <usual-configure-options>\n    make\n    sudo make install\n\n### Build for Windows\n\n    cd win32\n    nuget restore\n    msbuild\n\n**NOTE**: Requires `Build Tools for Visual Studio 2017` with components `Windows 8.1 SDK` and `VC++ 2015.3 v14.00 (v140) toolset` to be installed.\n\n## Quick build\n\nThe bootstrap.sh build script will download and build the required dependencies,\nproviding a quick and easy means of building kcat.\nInternet connectivity and wget/curl is required by this script.\nThe resulting kcat binary will be linked statically to avoid runtime\ndependencies.\n**NOTE**: Requires `curl` and `cmake` (for yajl) to be installed.\n\n    ./bootstrap.sh\n\n\n## Configuration\n\nAny librdkafka [configuration](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\nproperty can be set on the command line using `-X property=value`, or in\na configuration file specified by `-F <config-file>`.\n\nIf no configuration file was specified with `-F ..` on the command line,\nkcat will try the `$KCAT_CONFIG` or (deprecated) `$KAFKACAT_CONFIG`\nenvironment variable,\nand then the default configuration file `~/.config/kcat.conf` or\nthe (deprecated) `~/.config/kafkacat.conf`.\n\nConfiguration files are optional.\n\n\n## Examples\n\nHigh-level balanced KafkaConsumer: subscribe to topic1 and topic2\n(requires broker >=0.9.0 and librdkafka version >=0.9.1)\n\n    $ kcat -b mybroker -G mygroup topic1 topic2\n\n\nRead messages from stdin, produce to 'syslog' topic with snappy compression\n\n    $ tail -f /var/log/syslog | kcat -b mybroker -t syslog -z snappy\n\n\nRead messages from Kafka 'syslog' topic, print to stdout\n\n    $ kcat -b mybroker -t syslog\n\n\nProduce messages from file (one file is one message)\n\n    $ kcat -P -b mybroker -t filedrop -p 0 myfile1.bin /etc/motd thirdfile.tgz\n\n\nProduce messages transactionally (one single transaction for all messages):\n\n    $ kcat -P -b mybroker -t mytopic -X transactional.id=myproducerapp\n\n\nRead the last 2000 messages from 'syslog' topic, then exit\n\n    $ kcat -C -b mybroker -t syslog -p 0 -o -2000 -e\n\n\nConsume from all partitions from 'syslog' topic\n\n    $ kcat -C -b mybroker -t syslog\n\n\nOutput consumed messages in JSON envelope:\n\n    $ kcat -b mybroker -t syslog -J\n\n\nDecode Avro key (`-s key=avro`), value (`-s value=avro`) or both (`-s avro`) to JSON using schema from the Schema-Registry:\n\n    $ kcat -b mybroker -t ledger -s avro -r http://schema-registry-url:8080\n\n\nDecode Avro message value and extract Avro record's \"age\" field:\n\n    $ kcat -b mybroker -t ledger -s value=avro -r http://schema-registry-url:8080 | jq .payload.age\n\n\nDecode key as 32-bit signed integer and value as 16-bit signed integer followed by an unsigned byte followed by string:\n\n    $ kcat -b mybroker -t mytopic -s key='i$' -s value='hB s'\n\n\n*Hint: see `kcat -h` for all available deserializer options.*\n\n\nOutput consumed messages according to format string:\n\n    $ kcat -b mybroker -t syslog -f 'Topic %t[%p], offset: %o, key: %k, payload: %S bytes: %s\\n'\n\n\nRead the last 100 messages from topic 'syslog' with  librdkafka configuration parameter 'broker.version.fallback' set to '0.8.2.1' :\n\n    $ kcat -C -b mybroker -X broker.version.fallback=0.8.2.1 -t syslog -p 0 -o -100 -e\n\n\nProduce a tombstone (a \"delete\" for compacted topics) for key \"abc\" by providing an empty message value which `-Z` interpretes as NULL:\n\n    $ echo \"abc:\" | kcat -b mybroker -t mytopic -Z -K:\n\n\nProduce with headers:\n\n    $ echo \"hello there\" | kcat -b mybroker -P -t mytopic -H \"header1=header value\" -H \"nullheader\" -H \"emptyheader=\" -H \"header1=duplicateIsOk\"\n\n\nPrint headers in consumer:\n\n    $ kcat -b mybroker -C -t mytopic -f 'Headers: %h: Message value: %s\\n'\n\n\nEnable the idempotent producer, providing exactly-once and strict-ordering\n**producer** guarantees:\n\n    $ kcat -b mybroker -X enable.idempotence=true -P -t mytopic ....\n\n\nConnect to cluster using SSL and SASL PLAIN authentication:\n\n    $ kcat -b mybroker -X security.protocol=SASL_SSL -X sasl.mechanism=PLAIN -X sasl.username=myapikey -X sasl.password=myapisecret ...\n\n\nMetadata listing:\n\n```\n$ kcat -L -b mybroker\nMetadata for all topics (from broker 1: mybroker:9092/1):\n 3 brokers:\n  broker 1 at mybroker:9092\n  broker 2 at mybrokertoo:9092\n  broker 3 at thirdbroker:9092\n 16 topics:\n  topic \"syslog\" with 3 partitions:\n    partition 0, leader 3, replicas: 1,2,3, isrs: 1,2,3\n    partition 1, leader 1, replicas: 1,2,3, isrs: 1,2,3\n    partition 2, leader 1, replicas: 1,2, isrs: 1,2\n  topic \"rdkafkatest1_auto_49f744a4327b1b1e\" with 2 partitions:\n    partition 0, leader 3, replicas: 3, isrs: 3\n    partition 1, leader 1, replicas: 1, isrs: 1\n  topic \"rdkafkatest1_auto_e02f58f2c581cba\" with 2 partitions:\n    partition 0, leader 3, replicas: 3, isrs: 3\n    partition 1, leader 1, replicas: 1, isrs: 1\n  ....\n```\n\n\nJSON metadata listing\n\n    $ kcat -b mybroker -L -J\n\n\nPretty-printed JSON metadata listing\n\n    $ kcat -b mybroker -L -J | jq .\n\n\nQuery offset(s) by timestamp(s)\n\n    $ kcat -b mybroker -Q -t mytopic:3:2389238523 -t mytopic2:0:18921841\n\n\nConsume messages between two timestamps\n\n    $ kcat -b mybroker -C -t mytopic -o s@1568276612443 -o e@1568276617901\n\n\n\n## Running in Docker\n\nThe latest kcat docker image is `edenhill/kcat:1.7.1`, there's\nalso [Confluent's kafkacat docker images on Docker Hub](https://hub.docker.com/r/confluentinc/cp-kafkacat/).\n\nIf you are connecting to Kafka brokers also running on Docker you should specify the network name as part of the `docker run` command using the `--network` parameter. For more details of networking with Kafka and Docker [see this post](https://rmoff.net/2018/08/02/kafka-listeners-explained/).\n\nHere are two short examples of using kcat from Docker. See the [Docker Hub listing](https://hub.docker.com/r/confluentinc/cp-kafkacat/) and [kafkacat docs](https://docs.confluent.io/current/app-development/kafkacat-usage.html) for more details:\n\n**Send messages using [here doc](http://tldp.org/LDP/abs/html/here-docs.html):**\n\n```\ndocker run -it --rm \\\n        edenhill/kcat \\\n                -b kafka-broker:9092 \\\n                -t test \\\n                -K: \\\n                -P <<EOF\n\n1:{\"order_id\":1,\"order_ts\":1534772501276,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\n2:{\"order_id\":2,\"order_ts\":1534772605276,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\n3:{\"order_id\":3,\"order_ts\":1534772742276,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\nEOF\n```\n\n**Consume messages:**\n\n```\ndocker run -it --rm \\\n        edenhill/kcat \\\n           -b kafka-broker:9092 \\\n           -C \\\n           -f '\\nKey (%K bytes): %k\\t\\nValue (%S bytes): %s\\n\\Partition: %p\\tOffset: %o\\n--\\n' \\\n           -t test\n\nKey (1 bytes): 1\nValue (88 bytes): {\"order_id\":1,\"order_ts\":1534772501276,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\nPartition: 0    Offset: 0\n--\n\nKey (1 bytes): 2\nValue (89 bytes): {\"order_id\":2,\"order_ts\":1534772605276,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\nPartition: 0    Offset: 1\n--\n\nKey (1 bytes): 3\nValue (90 bytes): {\"order_id\":3,\"order_ts\":1534772742276,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\nPartition: 0    Offset: 2\n--\n% Reached end of topic test [0] at offset 3\n```\n\n\n## Run a mock Kafka cluster\n\nWith kcat you can spin up an ephemeral in-memory mock Kafka cluster\nthat you you can connect your Kafka applications to for quick\ntesting.\nThe mock cluster supports a reasonable subset of the Kafka\nprotocol, such as:\n\n * Producer\n * Idempotent Producer\n * Transactional Producer\n * Low-level consumer\n * High-level balanced consumer groups with offset commits\n * Topic Metadata and auto creation\n\n\nSpin the cluster by running kcat in the `-M` (for mock) mode:\n\n```bash\n\n# Create mock cluster with 3 brokers\n$ kcat -M 3\n...\nBROKERS=localhost:12345,localhost:46346,localhost:23599\n...\n```\n\nWhile kcat runs, let your Kafka applications connect to the mock cluster\nby configuring them with the `bootstrap.servers` emitted in the `BROKERS`\nline above.\n\nLet kcat run for as long as you need the cluster, then terminate it by\npressing `Ctrl-D`.\n\n\nSince the cluster runs all in memory, with no disk IO, it is quite suitable\nfor performance testing."
}
