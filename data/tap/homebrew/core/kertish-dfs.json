{
  "name": "kertish-dfs",
  "full_name": "kertish-dfs",
  "tap": "homebrew/core",
  "oldname": null,
  "oldnames": [],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "Kertish FileSystem and Cluster Administration CLI",
  "license": "GPL-3.0-only",
  "homepage": "https://github.com/freakmaxi/kertish-dfs",
  "versions": {
    "stable": "22.2.0147-532592",
    "head": "HEAD",
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://github.com/freakmaxi/kertish-dfs/archive/v22.2.0147.tar.gz",
      "tag": null,
      "revision": null,
      "checksum": "a13d55b3f48ed0e16b1add3a44587072b22d21a9f95c444893dbf92e19ee5cee"
    },
    "head": {
      "url": "https://github.com/freakmaxi/kertish-dfs.git",
      "branch": "master"
    }
  },
  "revision": 0,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "arm64_ventura": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:0dddc21a1847c5a75729b2eeafe55ae1916a12457cf727fb8b65c72ffc2c5f2f",
          "sha256": "0dddc21a1847c5a75729b2eeafe55ae1916a12457cf727fb8b65c72ffc2c5f2f"
        },
        "arm64_monterey": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:d3f56111dd712f97ff71450eb7f09577e5dc0108482de91064c57fc312609b4e",
          "sha256": "d3f56111dd712f97ff71450eb7f09577e5dc0108482de91064c57fc312609b4e"
        },
        "arm64_big_sur": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:e13c905be95c3f1d28bda7568811d2824e1125126f7a6f99a40aa937994a4fc8",
          "sha256": "e13c905be95c3f1d28bda7568811d2824e1125126f7a6f99a40aa937994a4fc8"
        },
        "ventura": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:28dc3892e2e452effd229c826451d2231862bf235c2fb4e32a12d5cebbf3626f",
          "sha256": "28dc3892e2e452effd229c826451d2231862bf235c2fb4e32a12d5cebbf3626f"
        },
        "monterey": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:08510e2f6ab40696f456d604a7d79372dfd585cc41c0b3703f2c09c7432a1483",
          "sha256": "08510e2f6ab40696f456d604a7d79372dfd585cc41c0b3703f2c09c7432a1483"
        },
        "big_sur": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:c9abefe4002333c21d00fe422d4d22b79fb665f923539e92e9da243eecaa0236",
          "sha256": "c9abefe4002333c21d00fe422d4d22b79fb665f923539e92e9da243eecaa0236"
        },
        "catalina": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:9faebdf52cff91734a78c929bb51cd460fffd6f8334c5e42c6db8ed8b337b023",
          "sha256": "9faebdf52cff91734a78c929bb51cd460fffd6f8334c5e42c6db8ed8b337b023"
        },
        "x86_64_linux": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/kertish-dfs/blobs/sha256:fa06124d7915154e90dcd81cd7a25888fa2baaecae519cc7cd8fed706f7b4fca",
          "sha256": "fa06124d7915154e90dcd81cd7a25888fa2baaecae519cc7cd8fed706f7b4fca"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [
    "go"
  ],
  "dependencies": [],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": null,
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/kertish-dfs.rb",
  "ruby_source_checksum": {
    "sha256": "f16b3a7a2cd8b3a6ff6d1d9ce51f1a8cb2ed5abbf419bca117722883e8e572c8"
  },
  "date_added": "2021-03-18T14:47:26+00:00",
  "readme": "# Kertish-DFS\n---\n[![Build Status](https://travis-ci.org/freakmaxi/kertish-dfs.svg?branch=master)](https://travis-ci.org/freakmaxi/kertish-dfs)\n![GitHub tag (latest SemVer)](https://img.shields.io/github/tag/freakmaxi/kertish-dfs.svg)\n[![Docker Pulls](https://img.shields.io/docker/pulls/freakmaxi/kertish-dfs.svg?maxAge=604800)](https://hub.docker.com/r/freakmaxi/kertish-dfs/)\n[![Go Report Card](https://goreportcard.com/badge/github.com/freakmaxi/kertish-dfs)](https://goreportcard.com/report/github.com/freakmaxi/kertish-dfs)\n\n<img src=\"http://imfrk.com/p/kertish-dfs/logo-point.png\" width=\"200px\">\n\n# Table of Contents\n- [Introduction](#introduction)\n- [Architecture](#architecture)\n- [Terminology](#terminology)\n- [Features](#features)\n- [System Requirements](#system-requirements)\n- [Installation Guide](#setup-description)\n  - [Sample Setup](#sample-setup)\n  - [Setup Using Docker](#setup-using-docker)\n  - [Setup Using Binaries](#setup-using-binaries)\n    - [Preparation](#preparation)\n    - [Setting Up Manager Node](#setting-up-manager-node)\n    - [Setting Up Head Node](#setting-up-head-node)\n    - [Setting Up Data Node](#setting-up-data-nodes)\n    - [Creating Clusters](#creating-clusters)\n    - [Manipulation File Storage](#manipulating-file-storage)\n- [Important Note](#one-last-important-note)\n\n---\n## Introduction\nKertish-dfs is a simple and highly scalable distributed file storage to store and serve billions of files. It is\ndeveloped to cover the expectation for mass file storage requirements in isolated networks.\n**It does not have security implementation.**\n\n#### What is it for?\nKertish-dfs is developed to cover the traditional file storage requirements in a scalable way. Software will use the same\npath-tree structure virtually to keep the files but will not stuck in hard-drive limits of the machine. It is possible\nto create a fail-safe, highly available fast and scalable file storage.\n\n#### Where can it be used?\nSystems where they need huge storage space requirements to keep all the files logically together. For example; \ncommon cloud storage for micro services, video storage for a streaming services, and the like...\n\n#### How shouldn't be used?\nKertish-dfs does not have any security implementation. For this reason, it is best to use it in a publicly isolated\nnetwork. Also it does not have file/folder permission integration, so user base limitations are also not available.\n\n#### How is the best usage?\nKertish-dfs is suitable to use as a back service of front services. It means, it is better not to allow users directly \nto the file manipulation.\n\n## Architecture\nKertish-dfs has 3 vital parts for a working farm. Manager-Node, Head-Node, Data-Node.\n- Manager-Node is responsible to handle data-node synchronisation and harmony for each cluster. It is handling the\nspace reservation, indexing data-node contents for fast search and find operations, health tracking of each data-node\nand optimize for the best performance every second, cluster balancing, check/repair/fix operations and synchronisation.\n- Head-Node(s) are responsible to handle read, write, delete, copy, move and merge operations. So, when you want to put\na file to Kertish-dfs, you will push the file to this node and it will handle the rest.\n- Data-Node(s) are responsible to hold the file chunks and serve it when it is requested as fast as possible.\n\nManager-Node and Data-Node must not accept direct request(s) coming from the outside of the Kertish-dfs farm. For \nfile/folder manipulation, only the access point should be Head-Node.\n\n**Here is a sample scenario;**\n1. User wants to upload a file (demo.mov - 125Mb) to /Foo/Bar as demo.mov\n2. Uses the Head-Node to upload file with /Foo/Bar/demo.mov header and the file content as binary\n3. Head-Node accepts the whole file and asks to Manager-Node where and how to put this file in data-node cluster(s)\n4. Manager-Node creates the best possible chunk map for Head-Node and reserve the spaces for those chunks in data-node(s)\n5. Head-Node divides the file into chunks and push the chunks to related data-node(s) and at the end commit the provided\nmap reservation if it has successful placement or discard the reservation to save the space.\n6. User gets success or error response with http status header.\n7. User can hold the file location in a database (/Foo/Bar/demo.mov)\n\n## Terminology\n- Farm: Whole Kertish-dfs with multi clusters. A farm can have one or more clusters with different sizes.\n- Cluster: A group of servers to hold data. A cluster can have one or more data node. \n- Data Node: A service running in a cluster to handle data manipulation requests. \n- Master: The server that has the latest version of the data blocks\n- Slave: The server that has the carbon copy of the master in the cluster.\n- Data Block: Data particle of the big data. Max size is 32mb. \n\n#### Summary\nKertish-dfs allows you to create data farm in distributed locations. Every data farm consist of clusters. Clusters\nare data banks. Every cluster has one or more data node. Always one data node works as master. Every data node\nadditions to that cluster will be used as backup of the master.\n\nData is always written to and deleted from the master data node in the cluster. However, reading request\nwill be balanced between slave data nodes. So in read sensitive environments, as much as new data node added\nto the cluster will help you to increase the response time. \n\n## Features\n- Scalable horizontally. You can add as much as cluster to grow the size of the storage. No total size limit.\n- Data shadowing. Copy operation won't increase the usage but let you the file/folder logically placed\n- Data particle stacking. Same data block won't be duplicated so theoretically total physical size may smaller than \nthe real size   \n- Fast traditional move/copy operations. It can take up to 5 seconds to move/copy a 1tb sized file in the dfs. \n- Multi tasking. Different request can work on the same folder.\n- Automated sync. Data nodes are smart enough to sync the data in the cluster.\n- Possible to take \"snapshot\" for marking the state of data-node and revert that moment if it requires.\n- REST architecture for file/folder manipulation.\n- Command-line `Admin` and `File Storage` tools\n\n## System Requirements\n\nKertish-dfs nodes has different hardware requirements to work flawless.\n\n- **Manager-Node** has redis, mongodb and locking-center TCP connections. In addition to that, it serves REST end-points\nfor head-node and data-node for management feedback requests. For these purposes, a powerful network connection is a\nmust. If you provide minimum 4 or more CPU Cores, it will significantly drop the response time between node communication.\nAccording to your Kertish farm setup, you may need 2GB or more memory. If you are serving many small files between\n1kb to 16mb, it is better to keep memory not less than 8 GB for 4 clusters with 8 data-nodes working master-slave logic\nand disk space size is between 350GB to 600GB. It is required to handle synchronization and repair operation handling\notherwise, it can fall to swap space which cause slow operation problem and if there is not any swap space configuration,\nit will lead the service to crash. **NOTE Always remember that Manager-Node is not scalable right now. It will work only\none instance. I'm working to make it scalable.**\n\n- **Head-Node** has mongodb and locking-center TCP connections. Also, it serves REST end-points for file storage\nmanipulations. It means, it is a good idea to have 200mbit or powerful network connection. Head-Node will cache the\nuploaded file to process. So, if you are uploading raw 32GB file to the Kertish-dfs, you should have a powerful\nmemory and swap space to hold the whole file in the memory. For this reason, you should have a powerful SSD Disk with a \nhuge swap space configured. **NOTE this logic will be extended in the future releases and will able to cover real-time\nuploading without caching.** If you are configuring the Kertish-dfs farm to serve just small files between 1kb to 2GB,\n4GB ram with 8GB swap space will be more than enough to cover expectations. On read wise, Head-Node does not cache\nanything, it transfers the data from the data-node to client. So memory is essential just for file uploads. Remember\nthat, Head-Node is scalable and you can put as much as Head-Node for file manipulation behind the load balancer. CPU is\nnot a big consideration. Minimum 2 or more CPU Cores will be sufficient. Head-Node does not do any serious calculation.\n\n- **Data-Node** has backend custom TCP ports to serve content. It makes REST requests to Manager-Node. For this reason,\neach node can have 100mbit or powerful network connection to serve files. Data-Node has optional caching feature to\ncache the most requested files to serve quickly. For a data-node with 500GB disk space without caching, 4GB memory is\nenough to operate. More memory will not change the response time or performance. If you consider to use caching, just\nput as much as memory top of 4GB to improve response time. It means, if you have a machine with 16GB memory,\n16GB-4GB = 12GB memory can be use for caching. Hard disk is a key point here. Better to use SSD for fast access and\nserve. HDD will be also okay if you are storing huge files because it will not have many small file chunks stored on\nthe disk. However, small files will create many chunks which will affect seek time of the disk head and that will lead\nyou a slow data-node. On CPU wise, it is not a critical topic. Minimum 2 or more CPU cores will be sufficient to serve\nfiles. On the other hand, slave nodes are periodically synchronize content with master and on that operation, CPU usage\ncan raise. So if you provide fast and more CPU core(s), synchronisation will finish quicker. \n\n## Setup Description\n\nKertish-dfs farm consist of minimum\n- 1 Manager Node\n- 1 Head Node\n- 1 Data Node\n- Mongo DB\n- Redis DSS\n- Locking-Center Server\n\n`Head Node` is for file storage interaction. When the data is wanted to access, the application should\nmake the request to this node. It works as REST service. File storage command-line tool communicate directly to \nhead node. Head node is scalable. Check `head-node` folder for details.\n\n`Manager Node` is for orchestrating the cluster(s). When the system should be setup first time or \nmanage farm for adding, removing cluster/node, this node will be used. Admin command-line tool \ncommunicate directly with manager node. Manager node is NOT scalable for now. Check `manager-node` folder for details.\n\n`Data Node` is to keep the data blocks. All the file data particles will be distributed on data nodes in\ndifferent clusters.\n\n**CAUTION: Deletion of a data node from cluster may cause the data lost and inconsistency.**\n---\n### Sample Setup\n\nI'll setup a farm using\n- 1 Manager Node\n- 1 Head Node\n- 4 Data Nodes in 2 Clusters working as Master/Slave\n- Mongo DB\n- Redis DSS\n- Locking-Center Server\n\nWhole setup will be done on the same machine. This is just for testing purpose. In real world, you \nwill need 6 servers for this setup. It is okay to keep the Mongo DB, Redis DSS, Locking-Center Server and Manager Node\nin the same machine if it covers the DB and DSS expectations. \n\nYou will not find the Mongo DB, Redis DSS, Locking-Center Server setup. Please follow the instruction on their web\nsites.\n- [Mongo DB](https://www.mongodb.com)\n- [Redis DSS](https://redis.io)\n- [Locking-Center Server](https://github.com/freakmaxi/locking-center)\n\n### Setup Using Docker\n\nThe docker hub page is [https://hub.docker.com/r/freakmaxi/kertish-dfs]\n\nYou can use the sample docker-compose file to kickstart the Kertish-dfs farm in docker container with 6 Data-Nodes \nworking in 3 Clusters as Master/Slave\n\n[https://github.com/freakmaxi/kertish-dfs/blob/master/docker-compose.yml]\n\n`docker-compose up` will make everything ready for you.\n\nDownload setup script from [https://github.com/freakmaxi/kertish-dfs/blob/master/kertish-docker-setup.sh]\n\n- Download Client-Tools for Kertish-dfs from [https://github.com/freakmaxi/kertish-dfs/releases] according to your OS\n- Give execution permission to the file `sudo chmod +x kertish-docker-setup`\n- Execute setup script.\n- type `y` and press `enter`\n\nYour Kertish-dfs farm is ready to go.\n\nPut any file using `krtfs` file storage tool. Ex:\n\n`./krtfs cp local:~/Downloads/demo.mov /demo.mov`\n\nJust change the path and file after `local:` according to the file in your system. Try to choose a file more than 70 Mb\nto see file chunk distribution between clusters. If file size is smaller than 32 Mb, it will be placed only in a cluster.\n\n`./krtfs ls -l` will give you an output similar like below\n\n```\nprocessing... ok.\ntotal 1\n- 87701kb 2020 Jun 22 22:07 demo.mov\n```\n\n---\n\n### Setup Using Binaries\n\n#### Preparation\n\n- Download the latest release of Kertish-dfs or compile it using the `create_release.sh` shell script file located under\nthe `-build-` folder.\n\n##### Setting Up Manager Node\n\nYou can take a look at [Manager-Node](https://github.com/freakmaxi/kertish-dfs/blob/master/manager-node) page to understand how it is working\n\n- Copy `kertish-manager` executable to `/usr/local/bin` folder on the system.\n- Give execution permission to the file `sudo chmod +x /usr/local/bin/kertish-manager`\n- Create an empty file in your user path, copy-paste the following and save the file\n```shell script\n#!/bin/sh\n\nexport MONGO_CONN=\"mongodb://root:pass@127.0.0.1:27017\" # Modify the values according to your setup\nexport REDIS_CONN=\"127.0.0.1:6379\"                      # Modify the values according to your setup\nexport LOCKING_CENTER=\"127.0.0.1:22119\"                 # Modify the values according to your setup\n/usr/local/bin/kertish-manager\n```\n- Give execution permission to the file `sudo chmod +x [Saved File Location]`\n- Execute the saved file.\n---\n##### Setting Up Head Node\n\nYou can take a look at [Head-Node](https://github.com/freakmaxi/kertish-dfs/blob/master/head-node) page to understand how it is working\n\n- Copy `kertish-head` executable to `/usr/local/bin` folder on the system.\n- Give execution permission to the file `sudo chmod +x /usr/local/bin/kertish-head`\n- Create an empty file in your user path, copy-paste the following and save the file\n```shell script\n#!/bin/sh\n\nexport MANAGER_ADDRESS=\"http://127.0.0.1:9400\" \nexport MONGO_CONN=\"mongodb://root:pass@127.0.0.1:27017\" # Modify the values according to your setup\nexport LOCKING_CENTER=\"127.0.0.1:22119\"                 # Modify the values according to your setup\n/usr/local/bin/kertish-head\n```\n- Give execution permission to the file `sudo chmod +x [Saved File Location]`\n- Execute the saved file.\n---\n##### Setting Up Data Node(s)\n\nYou can take a look at [Data-Node](https://github.com/freakmaxi/kertish-dfs/blob/master/data-node) page to understand how it is working\n\n- Copy `kertish-data` executable to `/usr/local/bin` folder on the system.\n- Give execution permission to the file `sudo chmod +x /usr/local/bin/kertish-data`\n- Create following folders - /opt/c1n1 - /opt/c1n2 - /opt/c2n1 - /opt/c2n2\n- Create an empty file on your user path named `dn-c1n1.sh`, copy-paste the following and save the file\n```shell script\n#!/bin/sh\n\nexport BIND_ADDRESS=\"127.0.0.1:9430\"\nexport MANAGER_ADDRESS=\"http://127.0.0.1:9400\"\nexport SIZE=\"1073741824\" # 1gb\nexport ROOT_PATH=\"/opt/c1n1\"\n/usr/local/bin/kertish-data\n```\n- Give execution permission to the file `sudo chmod +x [Saved File Location]`\n- Execute the saved file.\n\n---\n\n- Create an empty file on your user path named `dn-c1n2.sh`, copy-paste the following and save the file\n```shell script\n#!/bin/sh\n\nexport BIND_ADDRESS=\"127.0.0.1:9431\"\nexport MANAGER_ADDRESS=\"http://127.0.0.1:9400\"\nexport SIZE=\"1073741824\" # 1gb\nexport ROOT_PATH=\"/opt/c1n2\"\n/usr/local/bin/kertish-data\n```\n- Give execution permission to the file `sudo chmod +x [Saved File Location]`\n- Execute the saved file.\n\n---\n\n- Create an empty file on your user path named `dn-c2n1.sh`, copy-paste the following and save the file\n```shell script\n#!/bin/sh\n\nexport BIND_ADDRESS=\"127.0.0.1:9432\"\nexport MANAGER_ADDRESS=\"http://127.0.0.1:9400\"\nexport SIZE=\"1073741824\" # 1gb\nexport ROOT_PATH=\"/opt/c2n1\"\n/usr/local/bin/kertish-data\n```\n- Give execution permission to the file `sudo chmod +x [Saved File Location]`\n- Execute the saved file.\n\n---\n\n- Create an empty file on your user path named `dn-c2n2.sh`, copy-paste the following and save the file\n```shell script\n#!/bin/sh\n\nexport BIND_ADDRESS=\"127.0.0.1:9433\"\nexport MANAGER_ADDRESS=\"http://127.0.0.1:9400\"\nexport SIZE=\"1073741824\" # 1gb\nexport ROOT_PATH=\"/opt/c2n2\"\n/usr/local/bin/kertish-data\n```\n- Give execution permission to the file `sudo chmod 777 [Saved File Location]`\n- Execute the saved file.\n---\n##### Creating Clusters\n\n**IMPORTANT:** Data nodes sizes in the SAME CLUSTER have to be the same. You may have different servers with\ndifferent sized hard-drives. You should use the `SIZE` environment variable to align the storage spaces according to the\nthe server that has the smallest hard-drive size\n\n \n- Copy `krtadm` executable to `/usr/local/bin` folder on the system.\n- Give execution permission to the file `sudo chmod +x /usr/local/bin/krtadm`\n- Enter the following command\n`krtadm -create-cluster 127.0.0.1:9430,127.0.0.1:9431`\n- If everything went right, you should see an output like\n```\nCluster Details: eddd204e4cd23a14cb2f20c84299ee81\n      Data Node: 127.0.0.1:9430 (MASTER) -> 526f15a45bf813838accd7fff5040ad7\n      Data Node: 127.0.0.1:9431 (SLAVE) -> 205a634981bbad7d8a0651046ed1c87b\n\nok.\n```\n- Enter the following command to create the one another cluster\n`krtadm -create-cluster 127.0.0.1:9432,127.0.0.1:9433`\n- If everything went right, you should see something like this\n```\nCluster Details: 8f0e2bc02811f346d6cbb542c92d118d\n      Data Node: 127.0.0.1:9432 (MASTER) -> 7a758a149e4453b20a40b35f83f3a0e4\n      Data Node: 127.0.0.1:9433 (SLAVE) -> 6776201a0bb7daafb46c9e3931f0807e\n\nok.\n```\n---\n##### Manipulating File Storage\n\n- Copy `krtfs` executable to `/usr/local/bin` folder on the system.\n- Give execution permission to the file `sudo chmod +x /usr/local/bin/krtfs`\n- Enter the following command\n`krtfs ls -l`\noutput: \n```\nprocessing... ok.\ntotal 0\n```\n- Put a file from your local drive to dfs\n`krtfs cp local:/usr/local/bin/krtfs /krtfs`\noutput: \n```\nprocessing... ok.\n```\n- Enter the following command\n`krtfs ls -l`\noutput: \n```\nprocessing... ok.\ntotal 1\n-  7291kb 2020 Jan 13 05:30 krtfs\n```\n\nIf you get the same or similar outputs like here, congratulations! you successfully set up your Kertish-dfs. \n\n### One Last Important Note\n\nWhen you setup the cluster and the cluster starts taking data blocks, consider that cluster is as absolute. Deleting \nthe cluster will cause you data inconsistency and lost. Due to this reason, when you are creating the structure of\nyour farm, pay attention to your cluster setups the most. If you want to remove the cluster from the farm, consider to\nmove the cluster first from one point to another using `krtadm` client tool."
}
