{
  "name": "mods",
  "full_name": "mods",
  "tap": "homebrew/core",
  "oldname": null,
  "oldnames": [],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "GPT-4 on the command-line",
  "license": "MIT",
  "homepage": "https://github.com/charmbracelet/mods",
  "versions": {
    "stable": "0.2.0",
    "head": null,
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://github.com/charmbracelet/mods/archive/refs/tags/v0.2.0.tar.gz",
      "tag": null,
      "revision": null,
      "checksum": "553a405cd496b85fbcaa29aa2ad0c1170b55063f63903050eb886eb976e2b55e"
    }
  },
  "revision": 0,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "arm64_ventura": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/mods/blobs/sha256:0639fdbd9d75225ef3b4879efab74d3c3dbd3fedfd07bedc2931efae61fbc27d",
          "sha256": "0639fdbd9d75225ef3b4879efab74d3c3dbd3fedfd07bedc2931efae61fbc27d"
        },
        "arm64_monterey": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/mods/blobs/sha256:0639fdbd9d75225ef3b4879efab74d3c3dbd3fedfd07bedc2931efae61fbc27d",
          "sha256": "0639fdbd9d75225ef3b4879efab74d3c3dbd3fedfd07bedc2931efae61fbc27d"
        },
        "arm64_big_sur": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/mods/blobs/sha256:0639fdbd9d75225ef3b4879efab74d3c3dbd3fedfd07bedc2931efae61fbc27d",
          "sha256": "0639fdbd9d75225ef3b4879efab74d3c3dbd3fedfd07bedc2931efae61fbc27d"
        },
        "ventura": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/mods/blobs/sha256:4864ec2ea12f0cca35ccbc4d1b8080f9434e7ec9e4430e82def3629e5764baee",
          "sha256": "4864ec2ea12f0cca35ccbc4d1b8080f9434e7ec9e4430e82def3629e5764baee"
        },
        "monterey": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/mods/blobs/sha256:4864ec2ea12f0cca35ccbc4d1b8080f9434e7ec9e4430e82def3629e5764baee",
          "sha256": "4864ec2ea12f0cca35ccbc4d1b8080f9434e7ec9e4430e82def3629e5764baee"
        },
        "big_sur": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/mods/blobs/sha256:4864ec2ea12f0cca35ccbc4d1b8080f9434e7ec9e4430e82def3629e5764baee",
          "sha256": "4864ec2ea12f0cca35ccbc4d1b8080f9434e7ec9e4430e82def3629e5764baee"
        },
        "x86_64_linux": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/mods/blobs/sha256:54b74bc0ad2e733504c28a12451e471462adf905d769dc5267993b8ca6389e53",
          "sha256": "54b74bc0ad2e733504c28a12451e471462adf905d769dc5267993b8ca6389e53"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [
    "go"
  ],
  "dependencies": [],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": null,
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/mods.rb",
  "ruby_source_checksum": {
    "sha256": "ff44d52762901ccc93513ce70289c642f435b82d75bdb37117a665c7c6fa8d0c"
  },
  "date_added": "2023-05-22T16:01:24-04:00",
  "readme": "# Mods!\n\n<p>\n    <img src=\"https://github.com/charmbracelet/mods/assets/25087/5442bf46-b908-47af-bf4e-60f7c38951c4\" width=\"630\" alt=\"Mods product art and type treatment\"/>\n    <br>\n    <a href=\"https://github.com/charmbracelet/mods/releases\"><img src=\"https://img.shields.io/github/release/charmbracelet/mods.svg\" alt=\"Latest Release\"></a>\n    <a href=\"https://github.com/charmbracelet/mods/actions\"><img src=\"https://github.com/charmbracelet/mods/workflows/build/badge.svg\" alt=\"Build Status\"></a>\n</p>\n\nAI for the command line, built for pipelines.\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/347300c6-b382-462d-9f80-8520a27e14bb\" width=\"900\" alt=\"a GIF of mods running\"></p>\n\nLLM based AI is really good at interpreting the output of commands and\nreturning the results in CLI friendly text formats like Markdown. Mods is a\nsimple tool that makes it super easy to use AI on the command line and in your\npipelines. Mods works with [OpenAI](https://platform.openai.com/account/api-keys)\nand [LocalAI](https://github.com/go-skynet/LocalAI)\n\nTo get started, [install Mods](#installation) and check out some of the\nexamples below. Since Mods has built-in Markdown formatting, you may also want\nto grab [Glow](https://github.com/charmbracelet/glow) to give the output some\n_pizzazz_.\n\n## What Can It Do?\n\nMods works by reading standard in and prefacing it with a prompt supplied in\nthe `mods` arguments. It sends the input text to an LLM and prints out the\nresult, optionally asking the LLM to format the response as Markdown. This\ngives you a way to \"question\" the output of a command. Mods will also work on\nstandard in or an argument supplied prompt individually.\n\nBe sure to check out the [examples](examples.md).\n\n## Installation\n\nMods works with OpenAI compatible endpoints. By default, Mods is configured to\nsupport OpenAI's official API and a LocalAI installation running on port 8080.\nYou can configure additional endpoints in your settings file by running `mods -s`.\n\n### OpenAI\n\nMods uses GPT-4 by default and will fallback to GPT-3.5 Turbo if it's not\navailable. Set the `OPENAI_API_KEY` environment variable to a valid OpenAI key,\nwhich you can get [from here](https://platform.openai.com/account/api-keys).\n\nMods can also use the [Azure OpenAI](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service)\nservice. Set the `AZURE_OPENAI_KEY` environment variable and configure your\nAzure endpoint with `mods -s`.\n\n### LocalAI\n\nLocalAI allows you to run a multitude of models locally. Mods works with the\nGPT4ALL-J model as setup in [this tutorial](https://github.com/go-skynet/LocalAI#example-use-gpt4all-j-model).\nYou can define more LocalAI models and endpoints with `mods -s`.\n\n### Install Mods\n\n```bash\n# macOS or Linux\nbrew install charmbracelet/tap/mods\n\n# Arch Linux (btw)\nyay -S mods\n\n# Debian/Ubuntu\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg\necho \"deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *\" | sudo tee /etc/apt/sources.list.d/charm.list\nsudo apt update && sudo apt install mods\n\n# Fedora/RHEL\necho '[charm]\nname=Charm\nbaseurl=https://repo.charm.sh/yum/\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.charm.sh/yum/gpg.key' | sudo tee /etc/yum.repos.d/charm.repo\nsudo yum install mods\n```\n\nOr, download it:\n\n- [Packages][releases] are available in Debian and RPM formats\n- [Binaries][releases] are available for Linux, macOS, and Windows\n\n[releases]: https://github.com/charmbracelet/mods/releases\n\nOr, just install it with `go`:\n\n```sh\ngo install github.com/charmbracelet/mods@latest\n```\n\n## Saving conversations\n\nAll conversations are automatically saved, and can be identified by their latest\nprompt.\n\nA saved conversation has a SHA1 identifier and a title, similar to how Git\nworks.\n\nUnlike Git, conversations can be updated, keeping their id but having\na different title.\n\nCheck the [examples document](./Examples.md) for usage examples.\n\n## Settings\n\nMods lets you tune your query with a variety of settings. You can configure\nMods with `mods -s` or pass the settings as environment variables and flags.\n\n#### Model\n\n`-m`, `--model`, `MODS_MODEL`\n\nMods uses `gpt-4` with OpenAI by default but you can specify any model as long\nas your account has access to it or you have installed locally with LocalAI.\n\nYou can add new models to the settings with `mods -s`. You can also specify a\nmodel and an API endpoint with `-m` and `-a` to use models not in the settings\nfile.\n\n#### Title\n\n`-t`, --title`\n\nSet a custom save title for the conversation.\n\n#### Continue last\n\n`-C`, --continue-last`\n\nContinues the previous conversation.\n\n#### Continue\n\n`-c`, `--continue`\n\nContinue from the last response or a given title or SHA1.\n\n#### List\n\n`-l`, `--list`\n\nLists all saved conversations.\n\n#### Show\n\n`-s`, `--show`\n\nShow the saved conversation the given title or SHA1.\n\n#### Delete\n\n`--delete`\n\nDeletes the saved conversation with the given title or SHA1.\n\n#### Format As Markdown\n\n`-f`, `--format`, `MODS_FORMAT`\n\nAsk the LLM to format the response as markdown. You can edit the text passed to\nthe LLM with `mods -s` then changing the `format-text` value.\n\n#### Max Tokens\n\n`--max-tokens`, `MODS_MAX_TOKENS`\n\nMax tokens tells the LLM to respond in less than this number of tokens. LLMs\nare better at longer responses so values larger than 256 tend to work best.\n\n#### Temperature\n\n`--temp`, `MODS_TEMP`\n\nSampling temperature is a number between 0.0 and 2.0 and determines how\nconfident the model is in its choices. Higher values make the output more\nrandom and lower values make it more deterministic.\n\n#### TopP\n\n`--topp`, `MODS_TOPP`\n\nTop P is an alternative to sampling temperature. It's a number between 0.0 and\n2.0 with smaller numbers narrowing the domain from which the model will create\nits response.\n\n#### No Limit\n\n`--no-limit`, `MODS_NO_LIMIT`\n\nBy default Mods attempts to size the input to the maximum size the allowed by\nthe model. You can potentially squeeze a few more tokens into the input by\nsetting this but also risk getting a max token exceeded error from the OpenAI API.\n\n#### Include Prompt\n\n`-P`, `--prompt`, `MODS_INCLUDE_PROMPT`\n\nInclude prompt will preface the response with the entire prompt, both standard\nin and the prompt supplied by the arguments.\n\n#### Include Prompt Args\n\n`-p`, `--prompt-args`, `MODS_INCLUDE_PROMPT_ARGS`\n\nInclude prompt args will include _only_ the prompt supplied by the arguments.\nThis can be useful if your standard in content is long and you just a want a\nsummary before the response.\n\n#### Max Retries\n\n`--max-retries`, `MODS_MAX_RETRIES`\n\nThe maximum number of retries to failed API calls. The retries happen with an\nexponential backoff.\n\n#### Fanciness\n\n`--fanciness`, `MODS_FANCINESS`\n\nYour desired level of fanciness.\n\n#### Quiet\n\n`-q`, `--quiet`, `MODS_QUIET`\n\nOutput nothing to standard err.\n\n#### Reset Settings\n\n`--reset-settings`\n\nBackup your old settings file and reset everything to the defaults.\n\n#### No Cache\n\n`--no-cache`, `MODS_NO_CACHE`\n\nDisables conversation saving.\n\n#### HTTP Proxy\n\n`-x`, `--http-proxy`, `MODS_HTTP_PROXY`\n\nUse the HTTP proxy to the connect the API endpoints.\n\n## Whatcha Think?\n\nWe’d love to hear your thoughts on this project. Feel free to drop us a note.\n\n- [Twitter](https://twitter.com/charmcli)\n- [The Fediverse](https://mastodon.social/@charmcli)\n- [Discord](https://charm.sh/chat)\n\n## License\n\n[MIT](https://github.com/charmbracelet/mods/raw/main/LICENSE)\n\n---\n\nPart of [Charm](https://charm.sh).\n\n<a href=\"https://charm.sh/\"><img alt=\"The Charm logo\" width=\"400\" src=\"https://stuff.charm.sh/charm-badge.jpg\" /></a>\n\n<!--prettier-ignore-->\nCharm热爱开源 • Charm loves open source"
}
