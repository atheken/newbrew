{
  "name": "sparkey",
  "full_name": "sparkey",
  "tap": "homebrew/core",
  "oldname": null,
  "oldnames": [],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "Constant key-value store, best for frequent read/infrequent write uses",
  "license": "Apache-2.0",
  "homepage": "https://github.com/spotify/sparkey/",
  "versions": {
    "stable": "1.0.0",
    "head": null,
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://github.com/spotify/sparkey/archive/sparkey-1.0.0.tar.gz",
      "tag": null,
      "revision": null,
      "checksum": "d607fb816d71d97badce6301dd56e2538ef2badb6530c0a564b1092788f8f774"
    }
  },
  "revision": 1,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "arm64_ventura": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:b5b1dd6314393471c6d94f5cd9417add5c2fb18cae43a8aadb55bc27782ff521",
          "sha256": "b5b1dd6314393471c6d94f5cd9417add5c2fb18cae43a8aadb55bc27782ff521"
        },
        "arm64_monterey": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:17187c82468ffb126744c6ac8f4bc318a11234923dd70759ed0b2204d949516f",
          "sha256": "17187c82468ffb126744c6ac8f4bc318a11234923dd70759ed0b2204d949516f"
        },
        "arm64_big_sur": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:1b2b1cc05fd4af9994aa34e57bc8767bdb567455e27458e1a9ac38e340603c68",
          "sha256": "1b2b1cc05fd4af9994aa34e57bc8767bdb567455e27458e1a9ac38e340603c68"
        },
        "ventura": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:9f5be963c7067cdad5e22789cde8ca2eb21e9d587b538e57c228ab9b9bc90e1e",
          "sha256": "9f5be963c7067cdad5e22789cde8ca2eb21e9d587b538e57c228ab9b9bc90e1e"
        },
        "monterey": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:8df24a5323536f451f373746c1a1643ce31967502c3d8cb99807ffca49e53413",
          "sha256": "8df24a5323536f451f373746c1a1643ce31967502c3d8cb99807ffca49e53413"
        },
        "big_sur": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:6f469c28584124f46a7fa9835dee3311ef02d5a48f2d8fb8c8eb29f2c6688986",
          "sha256": "6f469c28584124f46a7fa9835dee3311ef02d5a48f2d8fb8c8eb29f2c6688986"
        },
        "catalina": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:b7e64101995d257df010edb67bafcd60745f09c7b0ebb9650c817eb7343f1899",
          "sha256": "b7e64101995d257df010edb67bafcd60745f09c7b0ebb9650c817eb7343f1899"
        },
        "mojave": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:438c323c343b7aade2da46316d24bcc4d5c7a95910a43914d70125af14a17636",
          "sha256": "438c323c343b7aade2da46316d24bcc4d5c7a95910a43914d70125af14a17636"
        },
        "high_sierra": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:4acbb473ce3be942b808af45789ccb7ede8199c728f7c381cd0dda1a105c8a9e",
          "sha256": "4acbb473ce3be942b808af45789ccb7ede8199c728f7c381cd0dda1a105c8a9e"
        },
        "x86_64_linux": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/sparkey/blobs/sha256:9b9dcde71d3afafa413ad52116f880796bd49f1a715159a794275fb4bd366de1",
          "sha256": "9b9dcde71d3afafa413ad52116f880796bd49f1a715159a794275fb4bd366de1"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [
    "autoconf",
    "automake",
    "libtool"
  ],
  "dependencies": [
    "snappy"
  ],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": null,
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/sparkey.rb",
  "ruby_source_checksum": {
    "sha256": "cc536010d07ebe4b5d3440d5faff5a5376aec572806bf9ede44f724e35dd694b"
  },
  "date_added": "2014-03-26T16:46:46+00:00",
  "readme": "Sparkey is a simple constant key/value storage library. It is mostly suited for\nread heavy systems with infrequent large bulk inserts. It includes both a C\nlibrary for working with sparkey index and log files (`libsparkey`), and a\ncommand line utility for getting info about and reading values from a sparkey\nindex/log (`sparkey`).\n\n### Travis\nContinuous integration with [travis](https://travis-ci.org/spotify/sparkey).\n\n[![Build Status](https://travis-ci.org/spotify/sparkey.svg?branch=master)](https://travis-ci.org/spotify/sparkey)\n\nDependencies\n------------\n\n* GNU build system (autoconf, automake, libtool)\n* [Snappy](http://google.github.io/snappy/)\n\nOptional\n\n* [Doxygen](http://www.doxygen.org/)\n\nBuilding\n--------\n    autoreconf --install\n    ./configure\n    make\n    make check\n\nAPI documentation can be generated with `doxygen`.\n\nInstalling\n----------\n    sudo make install && sudo ldconfig\n\nRelated projects\n----------------\n\n* [spotify/sparkey-python: Official python bindings](https://github.com/spotify/sparkey-python)\n* [spotify/sparkey-java: Official java implementation](https://github.com/spotify/sparkey-java)\n* [emnl/gnista: Unofficial ruby bindings](https://github.com/emnl/gnista)\n* [adamtanner/sparkey: Unofficial ruby bindings](https://github.com/adamtanner/sparkey)\n* [stephenmathieson/node-sparkey: Unofficial node bindings](https://github.com/stephenmathieson/node-sparkey)\n* [tiegz/sparkey-go: Unofficial go bindings](https://github.com/tiegz/sparkey-go)\n* [dflemstr/sparkey-rs: Unofficial rust bindings](https://github.com/dflemstr/sparkey-rs)\n\nDescription\n------------\nSparkey is an extremely simple persistent key-value store.\nYou could think of it as a read-only hashtable on disk and you wouldn't be far off.\nIt is designed and optimized for some server side usecases at Spotify but it is written to be completely generic\nand makes no assumptions about what kind of data is stored.\n\nSome key characteristics:\n\n* Supports data sizes up to 2^63 - 1 bytes.\n* Supports iteration, get, put, delete\n* Optimized for bulk writes.\n* Immutable hash table.\n* Any amount of concurrent independent readers.\n* Only allows one writer at a time per storage unit.\n* Cross platform storage file.\n* Low overhead per entry.\n* Constant read startup cost\n* Low number of disk seeks per read\n* Support for block level compression.\n* Data agnostic, it just maps byte arrays to byte arrays.\n\nWhat it's not:\n\n* It's not a distributed key value store - it's just a hash table on disk.\n* It's not a compacted data store, but that can be implemented on top of it, if needed.\n* It's not robust against data corruption.\n\nThe usecase we have for it at Spotify is serving data that rarely gets updated to\nusers or other services. The fast and efficient bulk writes makes it feasible to periodically rebuild the data,\nand the fast random access reads makes it suitable for high throughput low latency services.\nFor some services we have been able to saturate network interfaces while keeping cpu usage really low.\n\nLimitations\n-----------\nThe hash writing process requires memory allocation of num_entries * 16 * 1.3 bytes.\nThis means that you may run out of memory if trying to write a hash index for too many entries.\nFor instance, with 16 GB available RAM you may write 825 million entries.\n\nThis limitation has been removed in sparkey-java, but it has not yet been implemented in this version.\n\nUsage\n-----\nSparkey is meant to be used as a library embedded in other software. Take a look at the API documentation which gives examples on how to use it.\n\nLicense\n-------\nApache License, Version 2.0\n\nDesign\n------\nSparkey uses two files on disk to store its data. The first is the sparkey log file (.spl), which is simply a sequence of key value pairs.\nThis is an append-only file. You're not allowed to modify it in the middle, and you can't use more than one writer to append to it.\n\nThe other file is the sparkey index file (.spi) which is a just a hashtable pointing at entries in the log.\nThis is an immutable file, so you would typically only update it once you're done with your bulk appends.\n\nDoing a random lookup involves first finding the proper entry in the hashtable, and then doing a seek to the right offset in the log file.\nOn average, this means two disk seeks per access for a cold disk cache. If you mlock the index file, it goes down to one seek.\nFor some of our usecases, the total data set is less than the available RAM, so it makes sense to mlock everything.\n\nThe advantages of having two files instead of just one (another solution would be to append the hash table at the end) is that it's trivial to mlock one of the files and not the other. It also enables us to append more data to existing log files, even after it's already in use.\n\nHistory\n-------\nSparkey is the product of hackdays at Spotify, where our developers get to spend some of their time on anything they think is interesting.\n\nWe have several usecases where we need to serve large amounts of static data with high throughput and low latency.\nTo do this, we've built our own services, backed by various storage systems. Our flow consists of first generating large static storage files in our offline-systems, which then gets pushed out to the user facing services to serve the data.\n\nThe storage solutions we used for that have all served us well for a time, but they had limitations that became problematic.\n\n* We used to rely a lot on CDB (which is a really great piece of software). It performed blazingly quick and produces compact files. We only stopped using it when our data started growing close to the 4 GB limit\n* We also used (and still use) Tokyo Cabinet for a bunch of usecases. It performs really well for reading, but the write throughput really suffers when you can no longer keep the entire dataset in memory, and there were issues with opening the same file multiple times from the same process.\n\nWe needed a key-value store with the following characteristics:\n\n* random read throughput comparable to tokyo cabinet and cdb.\n* high throughput bulk writes.\n* low overhead.\n* high limit on data size.\n\nFor fun, we started hacking on a new key-value store on our internal hackdays, where developers get to work on whatever they're interested in.\nThe result was this project.\n\nPerformance\n-----------\nA very simple benchmark program is included - see src/bench.c.\nThe program is designed to be easily extended to measure other key value stores if anyone wants to.\nRunning it on a production-like server (Intel(R) Xeon(R) CPU L5630 @ 2.13GHz) we get the following:\n\n    Testing bulk insert of 1000 elements and 1000.000 random lookups\n      Candidate: Sparkey\n        creation time (wall):     0.00\n        creation time (cpu):      0.00\n        throughput (puts/cpusec): 1098272.88\n        file size:                28384\n        lookup time (wall):          0.50\n        lookup time (cpu):           0.58\n        throughput (lookups/cpusec): 1724692.62\n\n    Testing bulk insert of 1000.000 elements and 1000.000 random lookups\n      Candidate: Sparkey\n        creation time (wall):     0.50\n        creation time (cpu):      0.69\n        throughput (puts/cpusec): 1448618.25\n        file size:                34177984\n        lookup time (wall):          1.00\n        lookup time (cpu):           0.78\n        throughput (lookups/cpusec): 1284477.75\n\n    Testing bulk insert of 10.000.000 elements and 1000.000 random lookups\n      Candidate: Sparkey\n        creation time (wall):     7.50\n        creation time (cpu):      7.73\n        throughput (puts/cpusec): 1294209.62\n        file size:                413777988\n        lookup time (wall):          1.00\n        lookup time (cpu):           0.99\n        throughput (lookups/cpusec): 1014608.94\n\n    Testing bulk insert of 100.000.000 elements and 1000.000 random lookups\n      Candidate: Sparkey\n        creation time (wall):     82.00\n        creation time (cpu):      81.58\n        throughput (puts/cpusec): 1225726.75\n        file size:                4337777988\n        lookup time (wall):          2.00\n        lookup time (cpu):           1.98\n        throughput (lookups/cpusec): 503818.84\n\n    Testing bulk insert of 1000 elements and 1000.000 random lookups\n      Candidate: Sparkey compressed(1024)\n        creation time (wall):     0.00\n        creation time (cpu):      0.00\n        throughput (puts/cpusec): 1101445.38\n        file size:                19085\n        lookup time (wall):          3.50\n        lookup time (cpu):           3.30\n        throughput (lookups/cpusec): 303335.78\n\n    Testing bulk insert of 1000.000 elements and 1000.000 random lookups\n      Candidate: Sparkey compressed(1024)\n        creation time (wall):     0.50\n        creation time (cpu):      0.75\n        throughput (puts/cpusec): 1333903.25\n        file size:                19168683\n        lookup time (wall):          3.00\n        lookup time (cpu):           2.91\n        throughput (lookups/cpusec): 343833.28\n\n    Testing bulk insert of 10.000.000 elements and 1000.000 random lookups\n      Candidate: Sparkey compressed(1024)\n        creation time (wall):     8.50\n        creation time (cpu):      8.50\n        throughput (puts/cpusec): 1176634.25\n        file size:                311872187\n        lookup time (wall):          3.00\n        lookup time (cpu):           2.99\n        throughput (lookups/cpusec): 334490.22\n\n    Testing bulk insert of 100.000.000 elements and 1000.000 random lookups\n      Candidate: Sparkey compressed(1024)\n        creation time (wall):     90.50\n        creation time (cpu):      90.46\n        throughput (puts/cpusec): 1105412.00\n        file size:                3162865465\n        lookup time (wall):          3.50\n        lookup time (cpu):           3.60\n        throughput (lookups/cpusec): 277477.41\n\n\nFile format details\n-------------------\n\n### Log file format\nThe contents of the log file starts with a constant size header, describing some metadata about the log file.\nAfter that is just a sequence of entries, where each entry consists of a type, key and a value.\n\nEach entry begins with two Variable Length Quantity (VLQ) non-negative integers, A and B. The type is determined by the A.\nIf A = 0, it's a DELETE, and B represents the length of the key to delete.\nIf A > 0, it's a PUT and the key length is A - 1, and the value length is B.\n\n(It gets slightly more complex if block level compression is used, but we'll ignore that for now.)\n\n### Hash file format\nThe contents of the hash file starts with a constant size header, similarly to the log file.\nThe rest of the file is a hash table, represented as capacity * slotsize bytes.\nThe capacity is simply an upper bound of the number of live entries multiplied by a hash density factor > 1.0.\nThe default implementation uses density factor = 1.3.\nEach slot consists of two parts, the hash value part and the address.\nThe size of the hash value is either 4 or 8 bytes, depending on the hash algorithm. It currently uses murmurhash32 if the number of entries is small, and a 64 bit truncation of murmurhash128 if the number of entries is large.\nThe address is simply a reference into the log file, either as 4 or 8 bytes, depending on the size of the log file.\nThat means that the slotsize is usually 16 bytes for any reasonably large set of entries.\nBy storing the hash value itself in each slot we're wasting some space, but in return we can expect to avoid visiting the log file in most cases.\n\nHash lookup algorithm\n----------------------\nOne of few non-trivial parts in Sparkey is the way it does hash lookups. With hashtables there is always a risk of collisions. Even if the hash itself may not collide, the assigned slots may.\n\n(It recently came to my attention that the method described below is basically the same thing as Robin Hood hashing with backward shift deletion)\n\nLet's define displacement as the distance from the calculated optimal slot for a given hash to the slot it's actually placed in. Distance in this case is defined as the number of steps you need to move forward from your optimal slot to reach the actual slot.\n\nThe trivial and naive solution for this is to simply start with an empty hash table, move through the entries and put them in the first available slot, starting from the optimal slot, and this is almost what we do.\n\nIf we consider the average displacement, we can't really do better than that. We can however minimize the maximum displacement, which gives us some nice properties:\n\n* We can store the maximum displacement in the header, so we have an upper bound on traversals. We could possibly even use this information to binary search for the entry.\n* As soon as we reach an entry with higher displacement than the thing we're looking for, we can abort the lookup.\n\nIt's very easy to set up the hash table like this, we just need to do insertions into slots instead of appends. As soon as we reach a slot with a smaller displacement than our own, we shift the following slots up until the first empty slot one step and insert our own element.\n\nLet's illustrate it with an example - let's start off with an empty hash table with a capacity of 7:\n\n             hash value   log offset    optimal slot    displacement\n           +------------+------------+\n    slot 0 |            |            |\n           +------------+------------+\n    slot 1 |            |            |\n           +------------+------------+\n    slot 2 |            |            |\n           +------------+------------+\n    slot 3 |            |            |\n           +------------+------------+\n    slot 4 |            |            |\n           +------------+------------+\n    slot 5 |            |            |\n           +------------+------------+\n    slot 6 |            |            |\n           +------------+------------+\n\nWe add the key \"key0\" which happens to end up in slot 3, h(\"key0\") % 7 == 3. The slot is empty, so this is trivial:\n\n             hash value   log offset    optimal slot    displacement\n           +------------+------------+\n    slot 0 |            |            |\n           +------------+------------+\n    slot 1 |            |            |\n           +------------+------------+\n    slot 2 |            |            |\n           +------------+------------+\n    slot 3 | h(key0)    | 1          |  3               0\n           +------------+------------+\n    slot 4 |            |            |\n           +------------+------------+\n    slot 5 |            |            |\n           +------------+------------+\n    slot 6 |            |            |\n           +------------+------------+\n\nNow we add \"key1\" which happens to end up in slot 4:\n\n             hash value   log offset    optimal slot    displacement\n           +------------+------------+\n    slot 0 |            |            |\n           +------------+------------+\n    slot 1 |            |            |\n           +------------+------------+\n    slot 2 |            |            |\n           +------------+------------+\n    slot 3 | h(key0)    | 1          |  3               0\n           +------------+------------+\n    slot 4 | h(key1)    | 11         |  4               0\n           +------------+------------+\n    slot 5 |            |            |\n           +------------+------------+\n    slot 6 |            |            |\n           +------------+------------+\n\nNow we add \"key2\" which also wants to be in slot 3. This is a conflict, so we skip forward until we found a slot which has a lower displacement than our current displacement.\nWhen we find that slot, all following entries until the next empty slot move down one step:\n\n             hash value   log offset    optimal slot    displacement\n           +------------+------------+\n    slot 0 |            |            |\n           +------------+------------+\n    slot 1 |            |            |\n           +------------+------------+\n    slot 2 |            |            |\n           +------------+------------+\n    slot 3 | h(key0)    | 1          |  3               0\n           +------------+------------+\n    slot 4 | h(key2)    | 21         |  3               1\n           +------------+------------+\n    slot 5 | h(key1)    | 11         |  4               1\n           +------------+------------+\n    slot 6 |            |            |\n           +------------+------------+\n\nLet's add \"key3\" which maps to slot 5. We can't push down key1, because it already has displacement 1 and our current displacement for key3 is 0, so we have to move forward:\n\n             hash value   log offset    optimal slot    displacement\n           +------------+------------+\n    slot 0 |            |            |\n           +------------+------------+\n    slot 1 |            |            |\n           +------------+------------+\n    slot 2 |            |            |\n           +------------+------------+\n    slot 3 | h(key0)    | 1          |  3               0\n           +------------+------------+\n    slot 4 | h(key2)    | 21         |  3               1\n           +------------+------------+\n    slot 5 | h(key1)    | 11         |  4               1\n           +------------+------------+\n    slot 6 | h(key3)    | 31         |  5               1\n           +------------+------------+\nAdding \"key4\" for slot 3. It ends up in slot 5 with displacement 2 and key3 loops around to slot 0:\n\n             hash value   log offset    optimal slot    displacement\n           +------------+------------+\n    slot 0 | key(key3)  | 31         |  5               2\n           +------------+------------+\n    slot 1 |            |            |\n           +------------+------------+\n    slot 2 |            |            |\n           +------------+------------+\n    slot 3 | h(key0)    | 1          |  3               0\n           +------------+------------+\n    slot 4 | h(key2)    | 21         |  3               1\n           +------------+------------+\n    slot 5 | h(key4)    | 41         |  3               2\n           +------------+------------+\n    slot 6 | h(key1)    | 11         |  4               2\n           +------------+------------+\n\nNow, if we search for key123 which maps to slot 3 (but doesn't exist!), we can stop scanning as soon as we reach slot 6, because then the current displacement (3) is higher than the displacement of the entry at the current slot (2).\n\nCompression\n-----------\nSparkey also supports block level compression using google snappy. You select a block size which is then used to split the contents of the log into blocks. Each block is compressed independently with snappy. This can be useful if your bottleneck is file size and there is a lot of redundant data across adjacent entries. The downside of using this is that during lookups, at least one block needs to be decompressed. The larger blocks you choose, the better compression you may get, but you will also have higher lookup cost. This is a tradeoff that needs to be empirically evaluated for each use case."
}
