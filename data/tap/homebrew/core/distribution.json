{
  "name": "distribution",
  "full_name": "distribution",
  "tap": "homebrew/core",
  "oldname": null,
  "oldnames": [],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "Create ASCII graphical histograms in the terminal",
  "license": "GPL-2.0-only",
  "homepage": "https://github.com/time-less-ness/distribution",
  "versions": {
    "stable": "1.3",
    "head": "HEAD",
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://github.com/time-less-ness/distribution/archive/1.3.tar.gz",
      "tag": null,
      "revision": null,
      "checksum": "d7f2c9beeee15986d24d8068eb132c0a63c0bd9ace932e724cb38c1e6e54f95d"
    },
    "head": {
      "url": "https://github.com/time-less-ness/distribution.git",
      "branch": "master"
    }
  },
  "revision": 0,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "all": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/distribution/blobs/sha256:46e6afa7ee3cdc08f4fde478e6235b1df80813391abe507505e1452926d5aff2",
          "sha256": "46e6afa7ee3cdc08f4fde478e6235b1df80813391abe507505e1452926d5aff2"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [],
  "dependencies": [],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": null,
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/distribution.rb",
  "ruby_source_checksum": {
    "sha256": "d9807e67ee85b45ff5c065dccd9875228f99abb74b6244177bdbd578d43cffc2"
  },
  "date_added": "2014-12-18T08:41:03+00:00",
  "readme": "New Canonical Source\n====================\n\nI have taken employment such that it is difficult for me to be the owner of this project.\nI have transferred control to [the excellent engineer wizzat](https://github.com/wizzat/distribution).\nPlease contribute any PRs, issues, etc to that project rather than this one. Thank you,\nand it's been good serving you these past years.\n\ndistribution\n============\n\nShort, simple, direct scripts for creating character-based graphs in a\ncommand terminal. Status: stable. Features added very rarely.\n\n![diagram](https://raw.github.com/philovivero/distribution/master/screenshot.png?raw=true)\n\n\nPurpose\n=======\n\nTo generate graphs directly in the (ASCII-based) terminal. Most common use-case:\nif you type `long | list | of | commands | sort | uniq -c | sort -rn` in the terminal,\nthen you could replace the final `| sort | uniq -c | sort -rn` with `| distribution` and\nvery likely be happier with what you see.\n\nThe tool is mis-named. It was originally for generating histograms (a distribution\nof the frequency of input tokens) but it has since been expanded to generate\ntime-series graphs (or, in fact, graphs with any arbitrary \"x-axis\") as well.\n\nAt first, there will be only two scripts, the originals written in Perl and\nPython by Tim Ellis. Any other versions people are willing to create will be placed\nhere. The next likely candidate language is C++.\n\nThere are a few typical use cases for graphs in a terminal as we'll lay out here:\n\n## Tokenize and Graph\n\nA stream of ASCII bytes, tokenize it, tally the matching tokens, and graph\nthe result. For this example, assume \"file\" is a list of words with one word\nper line, so passing it to \"xargs\" makes it all-one-line.\n\n```\n$ cat file | xargs #put all words on one line\nthis is an arbitrary stream of tokens. this will be graphed with tokens pulled out. this is the first use case.\n$ cat file | xargs | distribution --tokenize=word --match=word --size=small -v\ntokens/lines examined: 25\n tokens/lines matched: 21\n       histogram keys: 17\n              runtime: 8.00ms\n      Key|Ct (Pct)    Histogram\n     this|3 (14.29%) -----------------------------------------------------------------------o\n       is|2  (9.52%) ---------------------------------------o\n   tokens|2  (9.52%) ---------------------------------------o\n  graphed|1  (4.76%) -------o\n     will|1  (4.76%) -------o\n```\n\n## Aggregate and Graph\n\nAn already-tokenised input, one-per-line, tally and graph them.\n\n```\n$ cat file | distribution -s=small -v\ntokens/lines examined: 21\n tokens/lines matched: 21\n       histogram keys: 18\n              runtime: 14.00ms\n      Key|Ct (Pct)    Histogram\n     this|3 (14.29%) -----------------------------------------------------------------------o\n       is|2  (9.52%) ---------------------------------------o\n  graphed|1  (4.76%) -------o\n       be|1  (4.76%) -------o\n```\n\n## Graph Already-Aggregated/Counted Tokens\n\nA list of tallies + tokens, one-per-line. Create a graph with labels. This matches\nthe typical output of several Unix commands such as \"du.\"\n\n```\n$ du -s /etc/*/ 2>/dev/null | distribution -g -v\ntokens/lines examined: 107\n tokens/lines matched: 5,176\n       histogram keys: 107\n              runtime: 2.00ms\n                    Key|Ct  (Pct)    Histogram\n              /etc/ssl/|920 (17.77%) -------------------------------------------\n           /etc/init.d/|396  (7.65%) -------------------\n              /etc/apt/|284  (5.49%) -------------\n   /etc/nagios-plugins/|224  (4.33%) -----------\n              /etc/cis/|188  (3.63%) ---------\n           /etc/nagios/|180  (3.48%) ---------\n            /etc/fonts/|172  (3.32%) --------\n              /etc/ssh/|172  (3.32%) --------\n          /etc/default/|164  (3.17%) --------\n    /etc/console-setup/|132  (2.55%) -------\n```\n\n## Graph a List of Integers\n\nA list of tallies only. Create a graph without labels. This is typical if you just\nhave a stream of numbers and wonder what they look like. The `--numonly` switch is\nused to toggle this behaviour.\n\nThere is a different project: https://github.com/holman/spark that will produce\nsimpler, more-compact graphs. By contrast, this project will produce rather\nlengthy and verbose graphs with far more resolution, which you may prefer.\n\n\nFeatures\n========\n\n   0. Configurable colourised output.\n   1. rcfile for your own preferred default commandline options.\n   2. Full Perl tokenising and regexp matching.\n   3. Partial-width Unicode characters for high-resolution charts.\n   4. Configurable chart sizes including \"fill up my whole screen.\"\n\n\nInstallation\n============\n\nIf you use homebrew, `brew install distribution` should do the trick, although\nif you already have Perl or Python installed, you can simply download the file\nand put it into your path.\n\nTo put the script into your homedir on the machine you plan to run the script:\n\n```\n$ wget https://raw.githubusercontent.com/philovivero/distribution/master/distribution.py\n$ sudo mv distribution.py /usr/local/bin/distribution\n$ alias worddist=\"distribution -t=word\"\n```\n\nIt is fine to place the script anywhere in your `$PATH`. The worddist alias is\nuseful for asking the script to tokenize the input for you eg `ls -alR | worddist`.\n\n\nOptions\n=======\n\n```\n  --keys=K       periodically prune hash to K keys (default 5000)\n  --char=C       character(s) to use for histogram character, some substitutions follow:\n        pl       Use 1/3-width unicode partial lines to simulate 3x actual terminal width\n        pb       Use 1/8-width unicode partial blocks to simulate 8x actual terminal width\n        ba       (▬) Bar\n        bl       (Ξ) Building\n        em       (—) Emdash\n        me       (⋯) Mid-Elipses\n        di       (♦) Diamond\n        dt       (•) Dot\n        sq       (□) Square\n  --color        colourise the output\n  --graph[=G]    input is already key/value pairs. vk is default:\n        kv       input is ordered key then value\n        vk       input is ordered value then key\n  --height=N     height of histogram, headers non-inclusive, overrides --size\n  --help         get help\n  --logarithmic  logarithmic graph\n  --match=RE     only match lines (or tokens) that match this regexp, some substitutions follow:\n        word     ^[A-Z,a-z]+$ - tokens/lines must be entirely alphabetic\n        num      ^\\d+$        - tokens/lines must be entirely numeric\n  --numonly[=N]  input is numerics, simply graph values without labels\n        actual   input is just values (default - abs, absolute are synonymous to actual)\n        diff     input monotonically-increasing, graph differences (of 2nd and later values)\n  --palette=P    comma-separated list of ANSI colour values for portions of the output\n                 in this order: regular, key, count, percent, graph. implies --color.\n  --rcfile=F     use this rcfile instead of $HOME/.distributionrc - must be first argument!\n  --size=S       size of histogram, can abbreviate to single character, overridden by --width/--height\n        small    40x10\n        medium   80x20\n        large    120x30\n        full     terminal width x terminal height (approximately)\n  --tokenize=RE  split input on regexp RE and make histogram of all resulting tokens\n        word     [^\\w] - split on non-word characters like colons, brackets, commas, etc\n        white    \\s    - split on whitespace\n  --width=N      width of the histogram report, N characters, overrides --size\n  --verbose      be verbose\n```\n\n\nSyslog Analysis Example\n=======================\n\nYou can grab out parts of your syslog ask the script to tokenize on non-word\ndelimiters, then only match words. The verbosity gives you some stats as it\nworks and right before it prints the histogram.\n\n```\n$ zcat /var/log/syslog*gz \\\n    | awk '{print $5\" \"$6}' \\\n    | distribution --tokenize=word --match=word --height=10 --verbose --char=o\ntokens/lines examined: 16,645    \n tokens/lines matched: 5,843\n       histogram keys: 92\n              runtime: 10.75ms\nVal       |Ct (Pct)      Histogram\nntop      |1818 (31.11%) ooooooooooooooooooooooooooooooooooooooooooooooooooooooo\nWARNING   |1619 (27.71%) ooooooooooooooooooooooooooooooooooooooooooooooooo\nkernel    |1146 (19.61%) ooooooooooooooooooooooooooooooooooo\nCRON      |153 (2.62%)   ooooo\nroot      |147 (2.52%)   ooooo\nmessage   |99 (1.69%)    ooo\nlast      |99 (1.69%)    ooo\nntpd      |99 (1.69%)    ooo\ndhclient  |88 (1.51%)    ooo\nTHREADMGMT|52 (0.89%)    oo\n```\n\n\nProcess List Example\n====================\n\nYou can start thinking of normal commands in new ways. For example, you can take\nyour \"ps ax\" output, get just the command portion, and do a word-analysis on it.\nYou might find some words are rather interesting. In this case, it appears Chrome\nis doing some sort of A/B testing and their commandline exposes that.\n\n```\n$ ps axww \\\n    | cut -c 28- \\\n    | distribution --tokenize=word --match=word --char='|' --width=90 --height=25\nVal                     |Ct (Pct)    Histogram\nusr                     |100 (6.17%) |||||||||||||||||||||||||||||||||||||||||||||||||||||\nlib                     |73 (4.51%)  ||||||||||||||||||||||||||||||||||||||\nbrowser                 |38 (2.35%)  ||||||||||||||||||||\nchromium                |38 (2.35%)  ||||||||||||||||||||\nP                       |32 (1.98%)  |||||||||||||||||\ndaemon                  |31 (1.91%)  |||||||||||||||||\nsbin                    |26 (1.60%)  ||||||||||||||\ngnome                   |23 (1.42%)  ||||||||||||\nbin                     |22 (1.36%)  ||||||||||||\nkworker                 |21 (1.30%)  |||||||||||\ntype                    |19 (1.17%)  ||||||||||\ngvfs                    |17 (1.05%)  |||||||||\nno                      |17 (1.05%)  |||||||||\nen                      |16 (0.99%)  |||||||||\nindicator               |15 (0.93%)  ||||||||\nchannel                 |14 (0.86%)  ||||||||\nbash                    |14 (0.86%)  ||||||||\nUS                      |14 (0.86%)  ||||||||\nlang                    |14 (0.86%)  ||||||||\nforce                   |12 (0.74%)  |||||||\npluto                   |12 (0.74%)  |||||||\nProxyConnectionImpact   |12 (0.74%)  |||||||\nHiddenExperimentB       |12 (0.74%)  |||||||\nConnectBackupJobsEnabled|12 (0.74%)  |||||||\nsession                 |12 (0.74%)  |||||||\n```\n\n\nGraphing Pre-Tallied Tokens Example\n===================================\n\nSometimes the output you have is already some keys with their counts. For\nexample the output of \"du\" or \"command | uniq -c\". In these cases, use the\n--graph (-g) option, which skips the parsing and tokenizing of the input.\n\nFurther, you can use very short versions of the options in case you don't like\ntyping a lot. The default character is \"+\" because it creates a type of grid\nsystem which makes it easy for the eye to trace right/left or up/down.\n\n```\n$ sudo du -sb /etc/* | distribution -w=90 -h=15 -g\nVal                   |Ct (Pct)         Histogram\n/etc/mateconf         |7780758 (44.60%) +++++++++++++++++++++++++++++++++++++++++++++++++\n/etc/brltty           |3143272 (18.02%) ++++++++++++++++++++\n/etc/apparmor.d       |1597915 (9.16%)  ++++++++++\n/etc/bash_completion.d|597836 (3.43%)   ++++\n/etc/mono             |535352 (3.07%)   ++++\n/etc/ssl              |465414 (2.67%)   +++\n/etc/ardour2          |362303 (2.08%)   +++\n/etc/X11              |226309 (1.30%)   ++\n/etc/ImageMagick      |202358 (1.16%)   ++\n/etc/init.d           |143281 (0.82%)   +\n/etc/ssh              |138042 (0.79%)   +\n/etc/fonts            |119862 (0.69%)   +\n/etc/sound            |112051 (0.64%)   +\n/etc/xdg              |111971 (0.64%)   +\n/etc/java-7-openjdk   |100414 (0.58%)   +\n```\n\n\nKeys in Natural Order Examples\n==============================\n\nThe output is separated between STDOUT and STDERR so you can sort the resulting\nhistogram by values. This is useful for time series or other cases where the\nkeys you're graphing are in some natural order. Note how the \"-v\" output still\nappears at the top.\n\n```\n$ cat NotServingRegionException-DateHour.txt \\\n    | distribution -v \\\n    | sort -n\ntokens/lines examined: 1,414,196    \n tokens/lines matched: 1,414,196\n       histogram keys: 453\n              runtime: 1279.30ms\nVal             |Ct (Pct)      Histogram\n   2012-07-13 03|38360 (2.71%) ++++++++++++++++++++++++\n   2012-07-28 21|18293 (1.29%) ++++++++++++\n   2012-07-28 23|20748 (1.47%) +++++++++++++\n   2012-07-29 06|15692 (1.11%) ++++++++++\n   2012-07-29 07|30432 (2.15%) +++++++++++++++++++\n   2012-07-29 08|76943 (5.44%) ++++++++++++++++++++++++++++++++++++++++++++++++\n   2012-07-29 09|54955 (3.89%) ++++++++++++++++++++++++++++++++++\n   2012-07-30 05|15652 (1.11%) ++++++++++\n   2012-07-30 09|40102 (2.84%) +++++++++++++++++++++++++\n   2012-07-30 10|21718 (1.54%) ++++++++++++++\n   2012-07-30 16|16041 (1.13%) ++++++++++\n   2012-08-01 09|22740 (1.61%) ++++++++++++++\n   2012-08-02 04|31851 (2.25%) ++++++++++++++++++++\n   2012-08-02 06|28748 (2.03%) ++++++++++++++++++\n   2012-08-02 07|18062 (1.28%) ++++++++++++\n   2012-08-02 20|23519 (1.66%) +++++++++++++++\n   2012-08-03 03|21587 (1.53%) ++++++++++++++\n   2012-08-03 08|33409 (2.36%) +++++++++++++++++++++\n   2012-08-03 10|15854 (1.12%) ++++++++++\n   2012-08-03 15|29828 (2.11%) +++++++++++++++++++\n   2012-08-03 16|20478 (1.45%) +++++++++++++\n   2012-08-03 17|39758 (2.81%) +++++++++++++++++++++++++\n   2012-08-03 18|19514 (1.38%) ++++++++++++\n   2012-08-03 19|18353 (1.30%) ++++++++++++\n   2012-08-03 22|18726 (1.32%) ++++++++++++\n__________________\n\n$ cat /usr/share/dict/words \\\n    | awk '{print length($1)}' \\\n    | distribution -c=: -w=90 -h=16 \\\n    | sort -n\nVal|Ct (Pct)       Histogram\n2 |182 (0.18%)    :\n3 |845 (0.85%)    ::::\n4 |3346 (3.37%)   ::::::::::::::::\n5 |6788 (6.84%)   :::::::::::::::::::::::::::::::\n6 |11278 (11.37%) ::::::::::::::::::::::::::::::::::::::::::::::::::::\n7 |14787 (14.91%) :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n8 |15674 (15.81%) ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n9 |14262 (14.38%) :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n10|11546 (11.64%) :::::::::::::::::::::::::::::::::::::::::::::::::::::\n11|8415 (8.49%)   :::::::::::::::::::::::::::::::::::::::\n12|5508 (5.55%)   :::::::::::::::::::::::::\n13|3236 (3.26%)   :::::::::::::::\n14|1679 (1.69%)   ::::::::\n15|893 (0.90%)    :::::\n16|382 (0.39%)    ::\n17|176 (0.18%)    :\n```\n\n\nMySQL Slow Query Log Analysis Examples\n======================================\n\nYou can sometimes gain interesting insights just by measuring the size of files\non your filesystem. Someone had captured slow-query-logs for every hour for\nmost of a day. Assuming they all compressed the same (a proper analysis would\nbe on uncompressed files - uncompressing them would have caused server impact -\nthis is good enough for illustration's sake), we can determine how much slow\nquery traffic appeared during a given hour of the day.\n\nSomething happened around 8am but otherwise the server seems to follow a normal\nsinusoidal pattern. But note because we're only analysing the file size, it\ncould be that 8am had the same number of slow queries, but that the queries\nthemselves were larger in byte-count. Or that the queries didn't compress as\nwell.\n\nAlso note that we aren't seeing every histogram entry here. Always take care to\nremember the tool is hiding low-frequency data from you unless you ask it to\ndraw uncommonly-tall histograms.\n\n```\n$ du -sb mysql-slow.log.*.gz | ~/distribution -g | sort -n\nVal                 |Ct (Pct)         Histogram\nmysql-slow.log.01.gz|1426694 (5.38%)  ++++++++++++++++++++\nmysql-slow.log.02.gz|1499467 (5.65%)  +++++++++++++++++++++\nmysql-slow.log.03.gz|1840727 (6.94%)  ++++++++++++++++++++++++++\nmysql-slow.log.04.gz|1570131 (5.92%)  ++++++++++++++++++++++\nmysql-slow.log.05.gz|1439021 (5.42%)  ++++++++++++++++++++\nmysql-slow.log.07.gz|859939 (3.24%)   ++++++++++++\nmysql-slow.log.08.gz|2976177 (11.21%) ++++++++++++++++++++++++++++++++++++++++++\nmysql-slow.log.09.gz|792269 (2.99%)   +++++++++++\nmysql-slow.log.11.gz|722148 (2.72%)   ++++++++++\nmysql-slow.log.12.gz|825731 (3.11%)   ++++++++++++\nmysql-slow.log.14.gz|1476023 (5.56%)  +++++++++++++++++++++\nmysql-slow.log.15.gz|2087129 (7.86%)  +++++++++++++++++++++++++++++\nmysql-slow.log.16.gz|1905867 (7.18%)  +++++++++++++++++++++++++++\nmysql-slow.log.19.gz|1314297 (4.95%)  +++++++++++++++++++\nmysql-slow.log.20.gz|802212 (3.02%)   ++++++++++++\n```\n\nA more-proper analysis on another set of slow logs involved actually getting\nthe time the query ran, pulling out the date/hour portion of the timestamp, and\ngraphing the result.\n\nAt first blush, it might appear someone had captured logs for various hours of\none day and at 10am for several days in a row. However, note that the Pct\ncolumn shows this is only about 20% of all data, which we can also conclude\nbecause there are 964 histogram entries, of which we're only seeing a couple\ndozen. This means something happened on July 31st that caused slow queries all\nday, and then 10am is a time of day when slow queries tend to happen. To test\nthis theory, we might re-run this with a \"--height=600\" (or even 900) to see\nnearly all the entries to get a more precise idea of what's going on.\n\n```\n$ zcat mysql-slow.log.*.gz \\\n    | fgrep Time: \\\n    | cut -c 9-17 \\\n    | ~/distribution --width=90 --verbose \\\n    | sort -n\ntokens/lines examined: 30,027    \n tokens/lines matched: 30,027\n       histogram keys: 964\n              runtime: 1224.58ms\nVal      |Ct (Pct)    Histogram\n120731 03|274 (0.91%) ++++++++++++++++++++++++++++++++++\n120731 04|210 (0.70%) ++++++++++++++++++++++++++\n120731 07|208 (0.69%) ++++++++++++++++++++++++++\n120731 08|271 (0.90%) +++++++++++++++++++++++++++++++++\n120731 09|403 (1.34%) +++++++++++++++++++++++++++++++++++++++++++++++++\n120731 10|556 (1.85%) ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n120731 11|421 (1.40%) +++++++++++++++++++++++++++++++++++++++++++++++++++\n120731 12|293 (0.98%) ++++++++++++++++++++++++++++++++++++\n120731 13|327 (1.09%) ++++++++++++++++++++++++++++++++++++++++\n120731 14|318 (1.06%) +++++++++++++++++++++++++++++++++++++++\n120731 15|446 (1.49%) ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n120731 16|397 (1.32%) ++++++++++++++++++++++++++++++++++++++++++++++++\n120731 17|228 (0.76%) ++++++++++++++++++++++++++++\n120801 10|515 (1.72%) +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n120803 10|223 (0.74%) +++++++++++++++++++++++++++\n120809 10|215 (0.72%) ++++++++++++++++++++++++++\n120810 10|210 (0.70%) ++++++++++++++++++++++++++\n120814 10|193 (0.64%) ++++++++++++++++++++++++\n120815 10|205 (0.68%) +++++++++++++++++++++++++\n120816 10|207 (0.69%) +++++++++++++++++++++++++\n120817 10|226 (0.75%) ++++++++++++++++++++++++++++\n120819 10|197 (0.66%) ++++++++++++++++++++++++\n```\n\nA typical problem for MySQL administrators is figuring out how many slow queries\nare taking how long. The slow query log can be quite verbose. Analysing it in a\nvisual nature can help. For example, there is a line that looks like this in the\nslow query log:\n\n```\n # Query_time: 5.260353  Lock_time: 0.000052  Rows_sent: 0  Rows_examined: 2414  Rows_affected: 1108  Rows_read: 2\n```\n\nIt might be useful to see how many queries ran for how long in increments of\ntenths of seconds. You can grab that third field and get tenth-second\nprecision with a simple awk command, then graph the result.\n\nIt seems interesting that there are spikes at 3.2, 3.5, 4, 4.3, 4.5 seconds.\nOne hypothesis might be that those are individual queries, each warranting its\nown analysis.\n\n```\n$ head -90000 mysql-slow.log.20120710 \\\n    | fgrep Query_time: \\\n    | awk '{print int($3 * 10)/10}' \\\n    | ~/distribution --verbose --height=30 --char='|o' \\\n    | sort -n\ntokens/lines examined: 12,269    \n tokens/lines matched: 12,269\n       histogram keys: 481\n              runtime: 12.53ms\nVal|Ct (Pct)     Histogram\n0  |1090 (8.88%) ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||o\n2  |1018 (8.30%) |||||||||||||||||||||||||||||||||||||||||||||||||||||||||o\n2.1|949 (7.73%)  |||||||||||||||||||||||||||||||||||||||||||||||||||||o\n2.2|653 (5.32%)  |||||||||||||||||||||||||||||||||||||o\n2.3|552 (4.50%)  |||||||||||||||||||||||||||||||o\n2.4|554 (4.52%)  |||||||||||||||||||||||||||||||o\n2.5|473 (3.86%)  ||||||||||||||||||||||||||o\n2.6|423 (3.45%)  ||||||||||||||||||||||||o\n2.7|394 (3.21%)  ||||||||||||||||||||||o\n2.8|278 (2.27%)  |||||||||||||||o\n2.9|189 (1.54%)  ||||||||||o\n3  |173 (1.41%)  |||||||||o\n3.1|193 (1.57%)  ||||||||||o\n3.2|200 (1.63%)  |||||||||||o\n3.3|138 (1.12%)  |||||||o\n3.4|176 (1.43%)  ||||||||||o\n3.5|213 (1.74%)  ||||||||||||o\n3.6|157 (1.28%)  ||||||||o\n3.7|134 (1.09%)  |||||||o\n3.8|121 (0.99%)  ||||||o\n3.9|96 (0.78%)   |||||o\n4  |110 (0.90%)  ||||||o\n4.1|80 (0.65%)   ||||o\n4.2|84 (0.68%)   ||||o\n4.3|90 (0.73%)   |||||o\n4.4|76 (0.62%)   ||||o\n4.5|93 (0.76%)   |||||o\n4.6|79 (0.64%)   ||||o\n4.7|71 (0.58%)   ||||o\n5.1|70 (0.57%)   |||o\n```\n\n\nApache Logs Analysis Example\n============================\n\nEven if you know sed/awk/grep, the built-in tokenizing/matching can be less\nverbose. Say you want to look at all the URLs in your Apache logs. People will\nbe doing GET /a/b/c /a/c/f q/r/s q/n/p. A and Q are the most common, so you can\ntokenize on / and the latter parts of the URL will be buried, statistically.\n\nBy tokenizing and matching using the script, you may also find unexpected\ncommon portions of the URL that don't show up in the prefix.\n\n```\n$ zcat access.log*gz \\\n    | awk '{print $7}' \\\n    | distribution -t=/ -h=15\nVal            |Ct (Pct)      Histogram\nArt            |1839 (16.58%) +++++++++++++++++++++++++++++++++++++++++++++++++\nRendered       |1596 (14.39%) ++++++++++++++++++++++++++++++++++++++++++\nBlender        |1499 (13.52%) ++++++++++++++++++++++++++++++++++++++++\nAznRigging     |760 (6.85%)   ++++++++++++++++++++\nMusic          |457 (4.12%)   ++++++++++++\nRingtones      |388 (3.50%)   +++++++++++\nCuteStance     |280 (2.52%)   ++++++++\nTraditional    |197 (1.78%)   ++++++\nTechnology     |171 (1.54%)   +++++\nCreativeExhaust|134 (1.21%)   ++++\nFractals       |127 (1.15%)   ++++\nrobots.txt     |125 (1.13%)   ++++\nRingtoneEP1.mp3|125 (1.13%)   ++++\nPoetry         |108 (0.97%)   +++\nRingtoneEP2.mp3|95 (0.86%)    +++\n```\n\nHere we had pulled apart our access logs and put them in TSV format for input\ninto Hive. The user agent string was in the 13th position. I wanted to just get an\noverall idea of what sort of user agents were coming to the site. I'm using the\nminimal argument size and my favorite \"character\" combo of \"|o\". I find it interesting\nthat there were only 474 unique word-based tokens in the input. Also, it's clear\nthat a large percentage of the visitors come with mobile devices now.\n\n```\n$ zcat weblog-2014-05.tsv.gz \\\n  | awk -F '\\t' '{print $13}' \\\n  | distribution -t=word -m=word -c='|o' -s=m -v\ntokens/lines examined: 28,062,913    \n tokens/lines matched: 11,507,407\n       histogram keys: 474\n              runtime: 15659.97ms\nVal        |Ct (Pct)       Histogram\nMozilla    |912852 (7.93%) ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||o\nlike       |722945 (6.28%) |||||||||||||||||||||||||||||||||||||||||||||||||||||||||o\nOS         |611503 (5.31%) ||||||||||||||||||||||||||||||||||||||||||||||||o\nAppleWebKit|605618 (5.26%) |||||||||||||||||||||||||||||||||||||||||||||||o\nGecko      |535620 (4.65%) ||||||||||||||||||||||||||||||||||||||||||o\nWindows    |484056 (4.21%) ||||||||||||||||||||||||||||||||||||||o\nNT         |483085 (4.20%) ||||||||||||||||||||||||||||||||||||||o\nKHTML      |356730 (3.10%) ||||||||||||||||||||||||||||o\nSafari     |355400 (3.09%) ||||||||||||||||||||||||||||o\nX          |347033 (3.02%) |||||||||||||||||||||||||||o\nMac        |344205 (2.99%) |||||||||||||||||||||||||||o\nappversion |300816 (2.61%) |||||||||||||||||||||||o\nType       |299085 (2.60%) |||||||||||||||||||||||o\nConnection |299085 (2.60%) |||||||||||||||||||||||o\nMobile     |282759 (2.46%) ||||||||||||||||||||||o\nCPU        |266837 (2.32%) |||||||||||||||||||||o\nNET        |247418 (2.15%) |||||||||||||||||||o\nCLR        |247418 (2.15%) |||||||||||||||||||o\nAspect     |242566 (2.11%) |||||||||||||||||||o\nRatio      |242566 (2.11%) |||||||||||||||||||o\n```\n\nAnd here we had a list of referrers in \"referrer [count]\" format. They were done one per day, but I wanted a count for January through September, so I used a shell glob to specify all those files for my 'cat'. Distribution will notice that it's getting the same key as previously and just add the new value, so the key \"x1\" can come in many times and we'll get the aggregate in the output. The referrers have been anonymized here since they are very specific to the company.\n\n```\n$ cat referrers-20140* | distribution -v -g=kv -s=m\ntokens/lines examined: 133,564    \n tokens/lines matched: 31,498,986\n       histogram keys: 14,882\n              runtime: 453.45ms\nVal    |Ct (Pct)          Histogram\nx1     |24313595 (77.19%) ++++++++++++++++++++++++++++++++++++++++++++++++++++\nx2     |3430278 (10.89%)  ++++++++\nx3     |1049996 (3.33%)   +++\nx4     |210083 (0.67%)    +\nx5     |179554 (0.57%)    +\nx6     |163158 (0.52%)    +\nx7     |129997 (0.41%)    +\nx8     |122725 (0.39%)    +\nx9     |120487 (0.38%)    +\nxa     |109085 (0.35%)    +\nxb     |99956 (0.32%)     +\nxc     |92208 (0.29%)     +\nxd     |90017 (0.29%)     +\nxe     |79416 (0.25%)     +\nxf     |70094 (0.22%)     +\nxg     |58089 (0.18%)     +\nxh     |52349 (0.17%)     +\nxi     |37002 (0.12%)     +\nxj     |36651 (0.12%)     +\nxk     |32860 (0.10%)     +\n```\n\nThis seems a really good time to use the --logarithmic option, since that top referrer\nis causing a loss of resolution on the following ones! I'll re-run this for one month.\n\n```\n$ cat referrers-201402* | distribution -v -g=kv -s=m -l\ntokens/lines examined: 23,517    \n tokens/lines matched: 5,908,765 \n       histogram keys: 5,888\n              runtime: 78.28ms\nVal  |Ct (Pct)         Histogram\nx1   |4471708 (75.68%) +++++++++++++++++++++++++++++++++++++++++++++++++++++\nx2   |670703 (11.35%)  ++++++++++++++++++++++++++++++++++++++++++++++\nx3   |203489 (3.44%)   ++++++++++++++++++++++++++++++++++++++++++\nx4   |43751 (0.74%)    +++++++++++++++++++++++++++++++++++++\nx5   |36211 (0.61%)    ++++++++++++++++++++++++++++++++++++\nx6   |34589 (0.59%)    ++++++++++++++++++++++++++++++++++++\nx7   |31279 (0.53%)    ++++++++++++++++++++++++++++++++++++\nx8   |29596 (0.50%)    +++++++++++++++++++++++++++++++++++\nx9   |23125 (0.39%)    +++++++++++++++++++++++++++++++++++\nxa   |21429 (0.36%)    ++++++++++++++++++++++++++++++++++\nxb   |19670 (0.33%)    ++++++++++++++++++++++++++++++++++\nxc   |19057 (0.32%)    ++++++++++++++++++++++++++++++++++\nxd   |18945 (0.32%)    ++++++++++++++++++++++++++++++++++\nxe   |18936 (0.32%)    ++++++++++++++++++++++++++++++++++\nxf   |16015 (0.27%)    +++++++++++++++++++++++++++++++++\nxg   |13115 (0.22%)    +++++++++++++++++++++++++++++++++\nxh   |12067 (0.20%)    ++++++++++++++++++++++++++++++++\nxi   |8485 (0.14%)     +++++++++++++++++++++++++++++++\nxj   |7694 (0.13%)     +++++++++++++++++++++++++++++++\nxk   |7199 (0.12%)     +++++++++++++++++++++++++++++++\n```\n\nGraphing a Series of Numbers Example\n====================================\n\nSuppose you just have a list of integers you want to graph. For example, you've\ncaptured a \"show global status\" for every second for 5 minutes, and you want to\ngrep out just one stat for the five-minute sample and graph it.\n\nOr, slightly more-difficult, you want to pull out the series of numbers and\nonly graph the difference between each pair (as in a monotonically-increasing\ncounter). The ```--numonly=``` option takes care of both these cases. This option\nwill override any \"height\" and simply graph all the numbers, since there's no\nfrequency to dictate which values are more important to graph than others.\n\nTherefore there's a lot of output, which is snipped in the example output that\nfollows. The \"val\" column is simply an ascending list of integers, so you can\ntell where output was snipped by the jumps in those values.\n\n```\n$ grep ^Innodb_data_reads globalStatus*.txt \\\n    | awk '{print $2}' \\\n    | distribution --numonly=mon --char='|+'\nVal|Ct (Pct)     Histogram\n1  |0 (0.00%)    +\n91 |15 (0.05%)   +\n92 |14 (0.04%)   +\n93 |30 (0.10%)   |+\n94 |11 (0.03%)   +\n95 |922 (2.93%)  |||||||||||||||||||||||||||||||||||||||||||||||||||||||||+\n96 |372 (1.18%)  |||||||||||||||||||||||+\n97 |44 (0.14%)   ||+\n98 |37 (0.12%)   ||+\n99 |110 (0.35%)  ||||||+\n100|18 (0.06%)   |+\n101|12 (0.04%)   +\n102|19 (0.06%)   |+\n103|164 (0.52%)  ||||||||||+\n200|62 (0.20%)   |||+\n201|372 (1.18%)  |||||||||||||||||||||||+\n202|228 (0.72%)  ||||||||||||||+\n203|43 (0.14%)   ||+\n204|917 (2.91%)  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||+\n205|64 (0.20%)   |||+\n206|178 (0.57%)  |||||||||||+\n207|90 (0.29%)   |||||+\n208|90 (0.29%)   |||||+\n209|101 (0.32%)  ||||||+\n453|0 (0.00%)    +\n454|0 (0.00%)    +\n```\n\nThe Python version eschews the header and superfluous \"key\" as the Perl version\nwill probably also soon do:\n\n```\n$ cat ~/tmp/numberSeries.txt | xargs\n01 05 06 09 12 22 28 32 34 30 37 44 48 54 63 70 78 82 85 88 89 89 90 92 95\n$ cat ~/tmp/numberSeries.txt \\\n    | ~/Dev/distribution/distribution.py --numonly -c='|o' -s=s\n   5  (0.39%) ||o\n   6  (0.47%) ||o\n   9  (0.70%) ||||o\n  12  (0.94%) |||||o\n  22  (1.71%) ||||||||||o\n  28  (2.18%) |||||||||||||o\n  32  (2.49%) |||||||||||||||o\n  34  (2.65%) ||||||||||||||||o\n  30  (2.34%) ||||||||||||||o\n  37  (2.88%) |||||||||||||||||o\n  44  (3.43%) ||||||||||||||||||||o\n  48  (3.74%) ||||||||||||||||||||||o\n  54  (4.21%) |||||||||||||||||||||||||o\n  63  (4.91%) |||||||||||||||||||||||||||||o\n  70  (5.46%) |||||||||||||||||||||||||||||||||o\n  78  (6.08%) ||||||||||||||||||||||||||||||||||||o\n  82  (6.39%) ||||||||||||||||||||||||||||||||||||||o\n  85  (6.63%) ||||||||||||||||||||||||||||||||||||||||o\n  88  (6.86%) |||||||||||||||||||||||||||||||||||||||||o\n  89  (6.94%) ||||||||||||||||||||||||||||||||||||||||||o\n  89  (6.94%) ||||||||||||||||||||||||||||||||||||||||||o\n  90  (7.01%) ||||||||||||||||||||||||||||||||||||||||||o\n  92  (7.17%) |||||||||||||||||||||||||||||||||||||||||||o\n  95  (7.40%) |||||||||||||||||||||||||||||||||||||||||||||o\n$ cat ~/tmp/numberSeries.txt \\\n\t| ~/Dev/distribution/distribution.py --numonly=diff -c='|o' -s=s\n   4  (4.26%) ||||||||||||||||||o\n   1  (1.06%) ||||o\n   3  (3.19%) |||||||||||||o\n   3  (3.19%) |||||||||||||o\n  10 (10.64%) |||||||||||||||||||||||||||||||||||||||||||||o\n   6  (6.38%) |||||||||||||||||||||||||||o\n   4  (4.26%) ||||||||||||||||||o\n   2  (2.13%) |||||||||o\n  -4 (-4.26%) o\n   7  (7.45%) |||||||||||||||||||||||||||||||o\n   7  (7.45%) |||||||||||||||||||||||||||||||o\n   4  (4.26%) ||||||||||||||||||o\n   6  (6.38%) |||||||||||||||||||||||||||o\n   9  (9.57%) ||||||||||||||||||||||||||||||||||||||||o\n   7  (7.45%) |||||||||||||||||||||||||||||||o\n   8  (8.51%) ||||||||||||||||||||||||||||||||||||o\n   4  (4.26%) ||||||||||||||||||o\n   3  (3.19%) |||||||||||||o\n   3  (3.19%) |||||||||||||o\n   1  (1.06%) ||||o\n   0  (0.00%) o\n   1  (1.06%) ||||o\n   2  (2.13%) |||||||||o\n   3  (3.19%) |||||||||||||o\n```\n\nHDFS DU Example\n===============\n\nHDFS files are often rather large, so I first change the numeric file size to\nmegabytes by dividing by 1048576. I must also change it to an int value, since\ndistribution doesn't currently deal with non-integer counts.\n\nAlso, we are pre-parsing the du output to give us only the megabytes count and\nthe final entry in the filename. `awk -F /` supports that.\n\n```\n$ hdfs dfs -du /apps/hive/warehouse/aedb/hitcounts_byday/cookie_type=shopper \\\n    | awk -F / '{print int($1/1048576) \" \" $8}' \\\n    | distribution -g -c='-~' --height=20 \\\n    | sort\n          Key|Ct      (Pct)   Histogram\ndt=2014-11-15|3265438 (2.53%) ----------------------------------------------~\ndt=2014-11-16|3241614 (2.51%) ----------------------------------------------~\ndt=2014-11-20|2964636 (2.29%) ------------------------------------------~\ndt=2014-11-21|3049912 (2.36%) -------------------------------------------~\ndt=2014-11-22|3292621 (2.55%) -----------------------------------------------~\ndt=2014-11-23|3319538 (2.57%) -----------------------------------------------~\ndt=2014-11-24|3070654 (2.38%) -------------------------------------------~\ndt=2014-11-25|3086090 (2.39%) --------------------------------------------~\ndt=2014-11-27|3113888 (2.41%) --------------------------------------------~\ndt=2014-11-28|3124426 (2.42%) --------------------------------------------~\ndt=2014-11-29|3431859 (2.66%) -------------------------------------------------~\ndt=2014-11-30|3391117 (2.62%) ------------------------------------------------~\ndt=2014-12-01|3167744 (2.45%) ---------------------------------------------~\ndt=2014-12-02|3134248 (2.43%) --------------------------------------------~\ndt=2014-12-03|3023733 (2.34%) -------------------------------------------~\ndt=2014-12-04|3022274 (2.34%) -------------------------------------------~\ndt=2014-12-05|3040776 (2.35%) -------------------------------------------~\ndt=2014-12-06|3054159 (2.36%) -------------------------------------------~\ndt=2014-12-09|3065252 (2.37%) -------------------------------------------~\ndt=2014-12-10|3316703 (2.57%) -----------------------------------------------~\n```\n\nRunning Tests\n=============\n\nThe `tests` directory contains sample input and output files, as well as a\nscript to verify expected output based on the sample inputs. To use it, first\nexport an environment variable called `distribution` that points to the\nlocation of your distribution executable. The script must be run from the `tests`\ndirectory. For example, the following will run tests against the Perl script\nand then against the Python script:\n\n    cd tests/\n\tdistribution=../distribution ./runTests.sh\n\tdistribution=../distribution.py ./runTests.sh\n\nThe `runTests.sh` script takes one optional argument, `-v`. This enables\nverbose mode, which prints out any differences in the stderr of the test runs,\nfor comparing diagnostic info.\n\nTo-Do List\n==========\n\nNew features are unlikely to be added, as the existing functionality is already\narguably a superset of what's necessary. Still, there are some things that need\nto be done.\n\n * No Time::HiRes Perl module? Don't die. Much harder than it should be.\n   Negated by next to-do.\n * Get scripts into package managers.\n\n\nPorting\n=======\n\nPerl and Python are fairly common, but I'm not sure 100% of systems out there\nhave them. A C/C++ port would be most welcome.\n\nIf you write a port, send me a pull request so I can include it in this repo.\n\nPort requirements: from the user's point of view, it's the exact same script.\nThey pass in the same options in the same way, and get the same output,\nbyte-for-byte if possible. This means you'll need (Perl) regexp support in your\nlanguage of choice. Also a hash map structure makes the implementation simple,\nbut more-efficient methods are welcome.\n\nI imagine, in order of nice-to-haveness:\n\n * C or C++\n * Go\n * Lisp\n * Ocaml\n * Java\n * Ruby\n\n List of known ports\n -------------------\n 1. [go-distribution](https://github.com/bradfordboyle/go-distribution)\n\n\nAuthors\n=======\n\n * Tim Ellis\n * Philo Vivero\n * Taylor Stearns"
}
