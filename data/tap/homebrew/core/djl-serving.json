{
  "name": "djl-serving",
  "full_name": "djl-serving",
  "tap": "homebrew/core",
  "oldname": null,
  "oldnames": [],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "This module contains an universal model serving implementation",
  "license": "Apache-2.0",
  "homepage": "https://github.com/deepjavalibrary/djl-serving",
  "versions": {
    "stable": "0.23.0",
    "head": null,
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://publish.djl.ai/djl-serving/serving-0.23.0.tar",
      "tag": null,
      "revision": null,
      "checksum": "31e2fa0581604614329e7e64fb2ef18eb29c5b20ba4bc5a8abacc494daf7b638"
    }
  },
  "revision": 0,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "all": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/djl-serving/blobs/sha256:2a5d5da8e206f9041f8eaa6378916bc2119f549b2af90ec7bfbd04f5032cebff",
          "sha256": "2a5d5da8e206f9041f8eaa6378916bc2119f549b2af90ec7bfbd04f5032cebff"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [],
  "dependencies": [
    "openjdk"
  ],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": {
    "run": [
      "/home/linuxbrew/.linuxbrew/opt/djl-serving/bin/djl-serving",
      "run"
    ],
    "run_type": "immediate",
    "keep_alive": {
      "always": true
    }
  },
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/djl-serving.rb",
  "ruby_source_checksum": {
    "sha256": "e39e3a427a04b90f9c2e28fe1b324244d020e29bac5e1a7272896dd957795aba"
  },
  "date_added": "2021-03-24T19:55:49+00:00",
  "readme": "# DJL Serving\n\n## Overview\n\nDJL Serving is a high performance universal stand-alone model serving solution powered by [DJL](https://djl.ai).\nIt takes a deep learning model, several models, or workflows and makes them available through an\nHTTP endpoint. It can serve the following model types out of the box:\n\n- PyTorch TorchScript model\n- TensorFlow SavedModel bundle\n- Apache MXNet model\n- ONNX model (CPU)\n- TensorRT model\n- Python script model\n\nYou can install extra extensions to enable the following models:\n\n- PaddlePaddle model\n- TFLite model\n- XGBoost model\n- LightGBM model\n- Sentencepiece model\n- fastText/BlazingText model\n\n## Key features\n\n- **Performance** - DJL serving running multithreading inference in a single JVM. Our benchmark shows\nDJL serving has higher throughput than most C++ model servers on the market\n- **Ease of use** - DJL serving can serve most models out of the box\n- **Easy to extend** - DJL serving plugins make it easy to add custom extensions\n- **Auto-scale** - DJL serving automatically scales up/down worker threads based on the load\n- **Dynamic batching** - DJL serving supports dynamic batching to increase throughput\n- **Model versioning** - DJL allows users to load different versions of a model on a single endpoint\n- **Multi-engine support** - DJL allows users to serve models from different engines at the same time\n\n## Installation\n\nFor macOS\n\n```\nbrew install djl-serving\n\n# Start djl-serving as service:\nbrew services start djl-serving\n\n# Stop djl-serving service\nbrew services stop djl-serving\n```\n\nFor Ubuntu\n\n```\ncurl -O https://publish.djl.ai/djl-serving/djl-serving_0.23.0-1_all.deb\nsudo dpkg -i djl-serving_0.23.0-1_all.deb\n```\n\nFor Windows\n\nWe are considering to create a `chocolatey` package for Windows. For the time being, you can\ndownload djl-serving zip file from [here](https://publish.djl.ai/djl-serving/serving-0.23.0.zip).\n\n```\ncurl -O https://publish.djl.ai/djl-serving/serving-0.23.0.zip\nunzip serving-0.23.0.zip\n# start djl-serving\nserving-0.23.0\\bin\\serving.bat\n```\n\n### Docker\n\nYou can also use docker to run DJL Serving:\n\n```\ndocker run -itd -p 8080:8080 deepjavalibrary/djl-serving\n```\n\n## Usage\n\nDJL Serving can be started from the command line.\nTo see examples, see the [starting page](serving/docs/starting.md).\n\n### More examples\n\n- [Serving a Python model](https://github.com/deepjavalibrary/djl-demo/tree/master/huggingface/python)\n- [Serving on Inferentia EC2 instance](https://github.com/deepjavalibrary/djl-demo/tree/master/huggingface/inferentia)\n- [Serving with docker](https://github.com/deepjavalibrary/djl-serving/tree/master/serving/docker)\n\n### More command line options\n\n```sh\ndjl-serving --help\nusage: djl-serving [OPTIONS]\n -f,--config-file <CONFIG-FILE>    Path to the configuration properties file.\n -h,--help                         Print this help.\n -m,--models <MODELS>              Models to be loaded at startup.\n -s,--model-store <MODELS-STORE>   Model store location where models can be loaded.\n -w,--workflows <WORKFLOWS>   Workflows to be loaded at startup.\n```\n\nSee [configuration](serving/docs/configuration.md) for more details about defining models, model-store, and workflows.\n\n## REST API\n\nDJL Serving uses a RESTful API for both inference and management calls.\n\nWhen DJL Serving starts up, it has two web services:\n\n* [Inference API](serving/docs/inference_api.md) - Used by clients to query the server and run models\n* [Management API](serving/docs/management_api.md) - Used to add, remove, and scale models on the server\n\nBy default, DJL Serving listens on port 8080 and is only accessible from localhost.\nPlease see [DJL Serving Configuration](serving/docs/configuration.md) for how to enable access from a remote host.\n\n## Architecture\n\nDetails about how DJL Serving is implemented can be found in the [architecture docs](serving/docs/architecture.md).\n\n# Plugin management\n\nDJL Serving supports plugins, user can implement their own plugins to enrich DJL Serving features.\nSee [DJL Plugin Management](serving/docs/plugin_management.md) for how to install plugins to DJL Serving.\n\n## Logging\nyou can set the logging level on the command-line adding a parameter for the JVM\n\n```sh\n-Dai.djl.logging.level={FATAL|ERROR|WARN|INFO|DEBUG|TRACE}\n```"
}
