{
  "name": "onnxruntime",
  "full_name": "onnxruntime",
  "tap": "homebrew/core",
  "oldname": null,
  "oldnames": [],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "Cross-platform, high performance scoring engine for ML models",
  "license": "MIT",
  "homepage": "https://github.com/microsoft/onnxruntime",
  "versions": {
    "stable": "1.15.1",
    "head": null,
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://github.com/microsoft/onnxruntime.git",
      "tag": "v1.15.1",
      "revision": "baeece44ba075009c6bfe95891a8c1b3d4571cb3",
      "checksum": null
    }
  },
  "revision": 0,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "arm64_ventura": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/onnxruntime/blobs/sha256:fa1614a946d6b74b70975191cc6d737371bda484702a6647b9c7653aba36293f",
          "sha256": "fa1614a946d6b74b70975191cc6d737371bda484702a6647b9c7653aba36293f"
        },
        "arm64_monterey": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/onnxruntime/blobs/sha256:0ede3332d4a1371429a8ebfdb91ec5783f7853fa963311cd5e8d9439d8a0c9f8",
          "sha256": "0ede3332d4a1371429a8ebfdb91ec5783f7853fa963311cd5e8d9439d8a0c9f8"
        },
        "arm64_big_sur": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/onnxruntime/blobs/sha256:c01b8c3bc163fd3848fda6b7b64d62117a614b1d6a0065e59fa72a0844375fe5",
          "sha256": "c01b8c3bc163fd3848fda6b7b64d62117a614b1d6a0065e59fa72a0844375fe5"
        },
        "ventura": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/onnxruntime/blobs/sha256:6bf6dcff3ce48adf469a9862c8c9287b749312c1974a67cf80c261c908537824",
          "sha256": "6bf6dcff3ce48adf469a9862c8c9287b749312c1974a67cf80c261c908537824"
        },
        "monterey": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/onnxruntime/blobs/sha256:f32d282661b950e8012b8fc8ca3785cf22f51f9242ce3e3518306caf5c9ce3ec",
          "sha256": "f32d282661b950e8012b8fc8ca3785cf22f51f9242ce3e3518306caf5c9ce3ec"
        },
        "big_sur": {
          "cellar": ":any",
          "url": "https://ghcr.io/v2/homebrew/core/onnxruntime/blobs/sha256:9372f3aff5069794712399f9af4fa170f1c96e7d8b33869baa860967e18b1533",
          "sha256": "9372f3aff5069794712399f9af4fa170f1c96e7d8b33869baa860967e18b1533"
        },
        "x86_64_linux": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/onnxruntime/blobs/sha256:f9b3e169149c0eff76f9ef8a9400f7ca6f585049f75669cc9c8b805818da44ab",
          "sha256": "f9b3e169149c0eff76f9ef8a9400f7ca6f585049f75669cc9c8b805818da44ab"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [
    "cmake",
    "python@3.11"
  ],
  "dependencies": [],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": null,
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/onnxruntime.rb",
  "ruby_source_checksum": {
    "sha256": "e353aea6231dda335f993d9f5dc55a49ce22309054464de1e72b7d5fd8596e85"
  },
  "date_added": "2019-11-26T00:49:34+00:00",
  "readme": "<p align=\"center\"><img width=\"50%\" src=\"docs/images/ONNX_Runtime_logo_dark.png\" /></p>\n\n**ONNX Runtime is a cross-platform inference and training machine-learning accelerator**.\n\n**ONNX Runtime inference** can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing)\n\n**ONNX Runtime training** can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-training)\n\n## Get Started & Resources\n\n* **General Information**: [onnxruntime.ai](https://onnxruntime.ai)\n\n* **Usage documention and tutorials**: [onnxruntime.ai/docs](https://onnxruntime.ai/docs)\n\n* **YouTube video tutorials**: [youtube.com/@ONNXRuntime](https://www.youtube.com/@ONNXRuntime)\n\n* [**Upcoming Release Roadmap**](https://github.com/microsoft/onnxruntime/wiki/Upcoming-Release-Roadmap)\n\n* **Companion sample repositories**:\n  - ONNX Runtime Inferencing: [microsoft/onnxruntime-inference-examples](https://github.com/microsoft/onnxruntime-inference-examples)\n  - ONNX Runtime Training: [microsoft/onnxruntime-training-examples](https://github.com/microsoft/onnxruntime-training-examples)\n\n## Builtin Pipeline Status\n\n|System|Inference|Training|\n|---|---|---|\n|Windows|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20CPU%20CI%20Pipeline?label=Windows+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=9)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20CI%20Pipeline?label=Windows+GPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=10)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20TensorRT%20CI%20Pipeline?label=Windows+GPU+TensorRT)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=47)||\n|Linux|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20CI%20Pipeline?label=Linux+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=11)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20Minimal%20Build%20E2E%20CI%20Pipeline?label=Linux+CPU+Minimal+Build)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=64)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20CI%20Pipeline?label=Linux+GPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=12)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20TensorRT%20CI%20Pipeline?label=Linux+GPU+TensorRT)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=45)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20OpenVINO%20CI%20Pipeline?label=Linux+OpenVINO)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=55)|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-ci-pipeline?label=Linux+CPU+Training)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=86)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-gpu-ci-pipeline?label=Linux+GPU+Training)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=84)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining/orttraining-ortmodule-distributed?label=Training+Distributed)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=148)|\n|Mac|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20CI%20Pipeline?label=MacOS+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=13)||\n|Android|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Android%20CI%20Pipeline?label=Android)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=53)||\n|iOS|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/iOS%20CI%20Pipeline?label=iOS)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=134)||\n|Web|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/ONNX%20Runtime%20Web%20CI%20Pipeline?label=Web)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=161)||\n|Other|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/onnxruntime-binary-size-checks-ci-pipeline?repoName=microsoft%2Fonnxruntime&label=Binary+Size+Check)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=187&repoName=microsoft%2Fonnxruntime)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/onnxruntime-python-checks-ci-pipeline?label=Python+Checks)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=164)||\n\n## Third-party Pipeline Status\n\n|System|Inference|Training|\n|---|---|---|\n|Linux|[![Build Status](https://github.com/Ascend/onnxruntime/actions/workflows/build-and-test.yaml/badge.svg)](https://github.com/Ascend/onnxruntime/actions/workflows/build-and-test.yaml)||\n\n## Data/Telemetry\n\nWindows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the [privacy statement](docs/Privacy.md) for more details.\n\n## Contributions and Feedback\n\nWe welcome contributions! Please see the [contribution guidelines](CONTRIBUTING.md).\n\nFor feature requests or bug reports, please file a [GitHub Issue](https://github.com/Microsoft/onnxruntime/issues).\n\nFor general discussion or questions, please use [GitHub Discussions](https://github.com/microsoft/onnxruntime/discussions).\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE)."
}
