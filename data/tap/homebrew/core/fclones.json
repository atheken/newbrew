{
  "name": "fclones",
  "full_name": "fclones",
  "tap": "homebrew/core",
  "oldname": null,
  "oldnames": [],
  "aliases": [],
  "versioned_formulae": [],
  "desc": "Efficient Duplicate File Finder",
  "license": "MIT",
  "homepage": "https://github.com/pkolaczk/fclones",
  "versions": {
    "stable": "0.32.1",
    "head": null,
    "bottle": true
  },
  "urls": {
    "stable": {
      "url": "https://github.com/pkolaczk/fclones/archive/refs/tags/v0.32.1.tar.gz",
      "tag": null,
      "revision": null,
      "checksum": "313d4dad30ed1db4d74abd78f30a7a9917c361918d2bc6d84c9d97a2a8c7c5cb"
    }
  },
  "revision": 0,
  "version_scheme": 0,
  "bottle": {
    "stable": {
      "rebuild": 0,
      "root_url": "https://ghcr.io/v2/homebrew/core",
      "files": {
        "arm64_ventura": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/fclones/blobs/sha256:9067e8185d41cf16f5d3bc29fdc2b3ce8f4edad185b3c9ec383ab06aa811cca0",
          "sha256": "9067e8185d41cf16f5d3bc29fdc2b3ce8f4edad185b3c9ec383ab06aa811cca0"
        },
        "arm64_monterey": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/fclones/blobs/sha256:03eff43c5ae76de8a591cc95a73ea5e0c65a689677d88e09f916da80606fa35b",
          "sha256": "03eff43c5ae76de8a591cc95a73ea5e0c65a689677d88e09f916da80606fa35b"
        },
        "arm64_big_sur": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/fclones/blobs/sha256:14f6717c129313c4d7547dbd9019045d2a1104c42bbe5885350c84ddc7ab15ee",
          "sha256": "14f6717c129313c4d7547dbd9019045d2a1104c42bbe5885350c84ddc7ab15ee"
        },
        "ventura": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/fclones/blobs/sha256:d5c29c8cde27f210b3980b15956bd1324c659e0c3651bf47db9038ff5295d901",
          "sha256": "d5c29c8cde27f210b3980b15956bd1324c659e0c3651bf47db9038ff5295d901"
        },
        "monterey": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/fclones/blobs/sha256:e36b45ce1b8df565f6d3cad2524457cc5ac8ce8052f37b515b2f9f22c1dd17ce",
          "sha256": "e36b45ce1b8df565f6d3cad2524457cc5ac8ce8052f37b515b2f9f22c1dd17ce"
        },
        "big_sur": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/fclones/blobs/sha256:6d92e888eb4d5636bf721e3cf195d2d8228a2ca3e1d4362dafa1e2d2469dc0a6",
          "sha256": "6d92e888eb4d5636bf721e3cf195d2d8228a2ca3e1d4362dafa1e2d2469dc0a6"
        },
        "x86_64_linux": {
          "cellar": ":any_skip_relocation",
          "url": "https://ghcr.io/v2/homebrew/core/fclones/blobs/sha256:8fb8949117746fa9a7d854b2f075c7ca21397f257ae46e39a6e1bfad5202a4e8",
          "sha256": "8fb8949117746fa9a7d854b2f075c7ca21397f257ae46e39a6e1bfad5202a4e8"
        }
      }
    }
  },
  "keg_only": false,
  "keg_only_reason": null,
  "options": [],
  "build_dependencies": [
    "rust"
  ],
  "dependencies": [],
  "test_dependencies": [],
  "recommended_dependencies": [],
  "optional_dependencies": [],
  "uses_from_macos": [],
  "uses_from_macos_bounds": [],
  "requirements": [],
  "conflicts_with": [],
  "conflicts_with_reasons": [],
  "link_overwrite": [],
  "caveats": null,
  "installed": [],
  "linked_keg": null,
  "pinned": false,
  "outdated": false,
  "deprecated": false,
  "deprecation_date": null,
  "deprecation_reason": null,
  "disabled": false,
  "disable_date": null,
  "disable_reason": null,
  "post_install_defined": false,
  "service": null,
  "tap_git_head": "4eeae4ea50839e967536ba646d5e0ed6fbcbad7f",
  "ruby_source_path": "Formula/fclones.rb",
  "ruby_source_checksum": {
    "sha256": "c6fc7e3b24e7d04c21b7aefd9325a082a03f70808e639d56909c8f15de761f42"
  },
  "date_added": "2022-05-01T19:36:17+00:00",
  "readme": "fclones\n===============================================\n**Efficient duplicate file finder and remover**\n\n[![CircleCI](https://circleci.com/gh/pkolaczk/fclones.svg?style=shield)](https://circleci.com/gh/pkolaczk/fclones)\n[![crates.io](https://img.shields.io/crates/v/fclones.svg)](https://crates.io/crates/fclones)\n[![Documentation](https://docs.rs/fclones/badge.svg)](https://docs.rs/fclones)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThis is the repo for command line fclones and its core libraries.\nFor the desktop frontend, see [fclones-gui](https://github.com/pkolaczk/fclones-gui).\n\n---\n\n`fclones` is a command line utility that identifies groups of identical files and gets rid \nof the file copies you no longer need. It comes with plenty of configuration options for controlling\nthe search scope and offers many ways of removing duplicates. For maximum flexibility,\nit integrates well with other Unix utilities like `find` and it speaks JSON, so you have a lot\nof control over the search and cleanup process.\n\n`fclones` treats your data seriously. You can inspect and modify the list of duplicate files before removing them.\nThere is also a `--dry-run` option that can tell you exactly what changes on the file system would be made.\n\n`fclones` has been implemented in Rust with a strong focus on high performance on modern hardware. \nIt employs several optimization techniques not present in many other programs. \nIt adapts to the type of the hard drive, orders file operations by physical data placement on HDDs, \nscans directory tree in parallel and uses prefix compression of paths to reduce memory consumption when working \nwith millions of files. It is also friendly to page-cache and does not push out your data out of cache.\nAs a result, `fclones` easily outperforms many other popular duplicate finders by a wide margin \non either SSD or HDD storage.\n\n`fclones` is available on a wide variety of operating systems, but it works best on Linux. \n\n- [Features](#features)\n- [Demo](#demo)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Algorithm](#the-algorithm)\n- [Tuning](#tuning)\n- [Benchmarks](#benchmarks)\n\n## Features\n* Identifying groups of identical files\n  - finding duplicate files\n  - finding files with more than N replicas\n  - finding unique files\n  - finding files with fewer than N replicas\n* Advanced file selection for reducing the amount of data to process\n  - scanning multiple directory roots\n  - can work with a list of files piped directly from standard input\n  - recursive/non-recursive file selection\n  - recursion depth limit\n  - filtering names and paths by extended UNIX globs\n  - filtering names and paths by regular expressions\n  - filtering by min/max file size\n  - proper handling of symlinks and hardlinks\n* Removing redundant data\n  - removing, moving or replacing files with soft or hard links\n  - removing redundant file data using native copy-on-write (reflink) support on some file systems \n  - selecting files for removal by path or name patterns  \n  - prioritizing files to remove by creation, modification, last access time or nesting level\n* High performance\n  - parallel processing capability in all I/O and CPU heavy stages\n  - automatic tuning of parallelism and access strategy based on device type (SSD vs HDD)\n  - low memory footprint thanks to heavily optimized path representation\n  - variety of fast non-cryptographic and cryptographic hash functions up to 512 bits wide\n  - doesn't push data out of the page-cache (Linux-only)\n  - optional persistent caching of file hashes\n  - accurate progress reporting   \n* Variety of output formats for easy further processing of results  \n  - standard text format\n    - groups separated by group headers with file size and hash \n    - one path per line in a group  \n  - optional `fdupes` compatibility (no headers, no indent, groups separated by blank lines)    \n  - machine-readable formats: `CSV`, `JSON`\n\n### Limitations\nCopy-on-write file data deduplication (reflink) is not supported on Windows. \n\nSome optimisations are not available on platforms other than Linux:\n  - ordering of file accesses by physical placement\n  - page-cache drop-behind\n  \n## Demo\nLet's first create some files:\n\n    $ mkdir test\n    $ cd test\n    $ echo foo >foo1.txt\n    $ echo foo >foo2.txt\n    $ echo foo >foo3.txt\n    $ echo bar >bar1.txt\n    $ echo bar >bar2.txt\n\nNow let's identify the duplicates:\n\n    $ fclones group . >dupes.txt\n    [2021-06-05 18:21:33.358] fclones:  info: Started grouping\n    [2021-06-05 18:21:33.738] fclones:  info: Scanned 7 file entries\n    [2021-06-05 18:21:33.738] fclones:  info: Found 5 (20 B) files matching selection criteria\n    [2021-06-05 18:21:33.738] fclones:  info: Found 4 (16 B) candidates after grouping by size\n    [2021-06-05 18:21:33.738] fclones:  info: Found 4 (16 B) candidates after grouping by paths and file identifiers\n    [2021-06-05 18:21:33.739] fclones:  info: Found 3 (12 B) candidates after grouping by prefix\n    [2021-06-05 18:21:33.740] fclones:  info: Found 3 (12 B) candidates after grouping by suffix\n    [2021-06-05 18:21:33.741] fclones:  info: Found 3 (12 B) redundant files\n\n    $ cat dupes.txt\n    # Report by fclones 0.12.0\n    # Timestamp: 2021-06-05 18:21:33.741 +0200\n    # Command: fclones group .\n    # Found 2 file groups\n    # 12 B (12 B) in 3 redundant files can be removed\n    7d6ebf613bf94dfd976d169ff6ae02c3, 4 B (4 B) * 2:\n        /tmp/test/bar1.txt\n        /tmp/test/bar2.txt\n    6109f093b3fd5eb1060989c990d1226f, 4 B (4 B) * 3:\n        /tmp/test/foo1.txt\n        /tmp/test/foo2.txt\n        /tmp/test/foo3.txt\n\nFinally we can replace the duplicates by soft links:\n\n    $ fclones link --soft <dupes.txt \n    [2021-06-05 18:25:42.488] fclones:  info: Started deduplicating\n    [2021-06-05 18:25:42.493] fclones:  info: Processed 3 files and reclaimed 12 B space\n\n    $ ls -l\n    total 12\n    -rw-rw-r-- 1 pkolaczk pkolaczk   4 cze  5 18:19 bar1.txt\n    lrwxrwxrwx 1 pkolaczk pkolaczk  18 cze  5 18:25 bar2.txt -> /tmp/test/bar1.txt\n    -rw-rw-r-- 1 pkolaczk pkolaczk 382 cze  5 18:21 dupes.txt\n    -rw-rw-r-- 1 pkolaczk pkolaczk   4 cze  5 18:19 foo1.txt\n    lrwxrwxrwx 1 pkolaczk pkolaczk  18 cze  5 18:25 foo2.txt -> /tmp/test/foo1.txt\n    lrwxrwxrwx 1 pkolaczk pkolaczk  18 cze  5 18:25 foo3.txt -> /tmp/test/foo1.txt\n\n## Installation\nThe code has been thoroughly tested on Ubuntu Linux 21.10.\nOther systems like Windows or Mac OS X and other architectures may work. \nHelp test and/or port to other platforms is welcome.\nPlease report successes as well as failures.      \n\n### Official Packages\n[Snap store](https://snapcraft.io/fclones) (Linux):\n\n    snap install fclones\n\n[Homebrew](https://formulae.brew.sh/formula/fclones) (macOS and Linux)\n\n    brew install fclones\n\nInstallation packages and binaries for some platforms \nare also attached directly to [Releases](https://github.com/pkolaczk/fclones/releases).\n\n### Third-party Packages\n* [Arch Linux](https://aur.archlinux.org/packages/fclones/) \n* [Alpine Linux](https://pkgs.alpinelinux.org/package/edge/testing/x86_64/fclones)\n* [NixOS](https://search.nixos.org/packages?channel=unstable&show=fclones&from=0&size=50&sort=relevance&type=packages&query=fclones)\n\n### Building from Source \n[Install Rust Toolchain](https://www.rust-lang.org/tools/install) and then run:\n\n    cargo install fclones\n\nThe build will write the binary to `.cargo/bin/fclones`. \n\n## Usage\n\n`fclones` offers separate commands for finding and removing files. This way, you can inspect\nthe list of found files before applying any modifications to the file system. \n\n  - `group` – identifies groups of identical files and prints them to the standard output\n  - `remove` – removes redundant files earlier identified by `group`\n  - `link` – replaces redundant files with links (default: hard links)\n  - `dedupe` – does not remove any files, but deduplicates file data by using native copy-on-write capabilities of the file system (reflink) \n\n### Finding Files\n\nFind duplicate, unique, under-replicated or over-replicated files in the current directory, \nincluding subdirectories:\n\n    fclones group .\n    fclones group . --unique \n    fclones group . --rf-under 3\n    fclones group . --rf-over 3\n\nYou can search in multiple directories:\n\n    fclones group dir1 dir2 dir3\n\nBy default, hidden files and files matching patterns listed in `.gitignore` and `.fdignore` are\nignored. To search all files, use:\n\n    fclones group --no-ignore --hidden dir\n\nLimit the recursion depth:\n    \n    fclones group . --depth 1   # scan only files in the current dir, skip subdirs\n    fclones group * --depth 0   # similar as above in shells that expand `*` \n\nCaution: Versions up to 0.10 did not descend into directories by default.\nIn those old versions, add `-R` flag to enable recursive directory walking.\n\nFinding files that match across two directory trees, without matching identical files\nwithin each tree:\n\n    fclones group --isolate dir1 dir2\n\nFinding duplicate files of size at least 100 MB: \n\n    fclones group . -s 100M\n\nFilter by file name or path pattern:\n\n    fclones group . --name '*.jpg' '*.png' \n                \nRun `fclones` on files selected by `find` (note: this is likely slower than built-in filtering):\n\n    find . -name '*.c' | fclones group --stdin --depth 0\n\nFollow symbolic links, but don't escape out of the home folder:\n\n    fclones group . -L --path '/home/**'\n    \nExclude a part of the directory tree from the scan:\n\n    fclones group / --exclude '/dev/**' '/proc/**'\n\n### Removing Files\nTo remove duplicate files, move them to a different place or replace them by links, \nyou need to send the report produced by `fclones group` to the standard input \nof `fclones remove`, `fclones move` or `fclones link` command.\nThe report format is detected automatically. Currently, `default` and `json` report \nformats are supported. \n\nAssuming the list of duplicates has been saved in file `dupes.txt`, the following commands would remove\nthe redundant files: \n\n    fclones link <dupes.txt             # replace with hard links\n    fclones link -s <dupes.txt          # replace with symbolic links\n    fclones move target_dir <dupes.txt  # move to target_dir  \n    fclones remove <dupes.txt           # remove totally\n    \n\nIf you prefer to do everything at once without storing the list of groups in a file, you can pipe:\n\n    fclones group . | fclones link\n\nTo select the number of files to preserve, use the `-n`/`--rf-over` option.\nBy default, it is set to the value used when running `group` (which is 1 if it wasn't set explicitly). \nTo leave 2 replicas in each group, run: \n\n    fclones remove -n 2 <dupes.txt\n\nBy default, `fclones` follows the order of files specified in the input file. It keeps the files given at the beginning\nof each list, and removes / replaces the files given at the end of each list. It is possible to change that \norder by `--priority` option, for example:\n\n    fclones remove --priority newest <dupes.txt        # remove the newest replicas\n    fclones remove --priority oldest <dupes.txt        # remove the oldest replicas\n\nFor more priority options, see `fclones remove --help`.\n\nIt is also possible to restrict removing files to only files with names or paths matching a pattern:\n\n    fclones remove --name '*.jpg' <dupes.txt       # remove only jpg files\n    fclones remove --path '/trash/**' <dupes.txt   # remove only files in the /trash folder\n\nIf it is easier to specify a pattern for files which you do *not* want to remove, then use one of `keep` options:\n\n    fclones remove --keep-name '*.mov' <dupes.txt           # never remove mov files\n    fclones remove --keep-path '/important/**' <dupes.txt   # never remove files in the /important folder\n\nTo make sure you're not going to remove wrong files accidentally, use `--dry-run` option.\nThis option prints all the commands that would be executed, but it doesn't actually execute them:\n\n    fclones link --soft <dupes.txt --dry-run 2>/dev/null\n\n    mv /tmp/test/bar2.txt /tmp/test/bar2.txt.jkXswbsDxhqItPeOfCXsWN4d\n    ln -s /tmp/test/bar1.txt /tmp/test/bar2.txt\n    rm /tmp/test/bar2.txt.jkXswbsDxhqItPeOfCXsWN4d\n    mv /tmp/test/foo2.txt /tmp/test/foo2.txt.ze1hvhNjfre618TkRGUxJNzx\n    ln -s /tmp/test/foo1.txt /tmp/test/foo2.txt\n    rm /tmp/test/foo2.txt.ze1hvhNjfre618TkRGUxJNzx\n    mv /tmp/test/foo3.txt /tmp/test/foo3.txt.ttLAWO6YckczL1LXEsHfcEau\n    ln -s /tmp/test/foo1.txt /tmp/test/foo3.txt\n    rm /tmp/test/foo3.txt.ttLAWO6YckczL1LXEsHfcEau\n\n### Handling links\nFiles linked by symbolic links or hard links are not treated as duplicates.\nYou can change this behavior by setting the following flags:\n * When `--isolate` is set:\n   * links residing in different directory trees are treated as duplicates, \n   * links residing in the same directory tree are counted as a single replica.\n * When `--match-links` is set, fclones treats all linked files as duplicates.\n\nConsider the following directory structure, where all files are hard links sharing the same content:\n\n    dir1:\n      - file1\n      - file2\n    dir2:\n      - file3\n      - file4\n      \nBecause all files are essentially the same data, they will end up in the same file group, but\nthe actual number of replicas present in that file group will differ depending on the flags given:\n\n| Command                                 | Number of replicas | Group reported   | Files to remove     |\n|-----------------------------------------|--------------------|------------------|---------------------|\n| `fclones group dir1 dir2`               | 1                  | No               |                     |\n| `fclones group dir1 dir2 --isolate`     | 2                  | Yes              | file3, file4        | \n| `fclones group dir1 dir2 --match-links` | 4                  | Yes              | file2, file3, file4 |\n\n#### Symbolic links\nThe `group` command ignores symbolic links to files unless at least `--follow-links` \nor `--symbolic-links` flag is set. If only `--follow-links` is set, symbolic links to files\nare followed and resolved to their targets. \nIf `--symbolic-links` is set, symbolic links to files are not followed, \nbut treated as hard links and potentially reported in the output report.\nWhen both `--symbolic-links` and `--follow-links` are set, symbolic links to directories are followed,\nbut symbolic links to files are treated as hard links.\n\n**Caution**: Using `--match-links` together with `--symbolic-links` is very dangerous. \nIt is easy to end up deleting the only regular file you have, and to be left\nwith a bunch of orphan symbolic links. \n\n### Preprocessing Files\nUse `--transform` option to safely transform files by an external command.\nBy default, the transformation happens on a copy of file data, to avoid accidental data loss.\nNote that this option may significantly slow down processing of a huge number of files, \nbecause it invokes the external program for each file.\n\nThe following command will strip exif before matching duplicate jpg images:\n\n    fclones group . --name '*.jpg' -i --transform 'exiv2 -d a $IN' --in-place     \n    \n### Other    \n    \nList more options:\n    \n    fclones [command] -h      # short help\n    fclones [command] --help  # detailed help\n\n### Path Globbing\n`fclones` understands a subset of Bash Extended Globbing.\nThe following wildcards can be used:\n- `?`         matches any character except the directory separator\n- `[a-z]`     matches one of the characters or character ranges given in the square brackets\n- `[!a-z]`    matches any character that is not given in the square brackets\n- `*`         matches any sequence of characters except the directory separator\n- `**`        matches any sequence of characters including the directory separator\n- `{a,b}`     matches exactly one pattern from the comma-separated patterns given inside the curly brackets\n- `@(a|b)`    same as `{a,b}`\n- `?(a|b)`    matches at most one occurrence of the pattern inside the brackets\n- `+(a|b)`    matches at least occurrence of the patterns given inside the brackets\n- `*(a|b)`    matches any number of occurrences of the patterns given inside the brackets\n- `\\`         escapes wildcards on Unix-like systems, e.g. `\\?` would match `?` literally\n- `^`         escapes wildcards on Windows, e.g. `^?` would match `?` literally\n\n#### Caution\n\n* On Unix-like systems, when using globs, one must be very careful to avoid accidental expansion of globs by the shell.\n  In many cases having globs expanded by the shell instead of by `fclones` is not what you want. In such cases, you\n  need to quote the globs:\n    \n      fclones group . --name '*.jpg'       \n       \n* On Windows, the default shell doesn't remove quotes before passing the arguments to the program, \n  therefore you need to pass globs unquoted:\n  \n      fclones group . --name *.jpg\n      \n* On Windows, the default shell doesn't support path globbing, therefore wildcard characters such as * and ? used \n  in paths will be passed literally, and they are likely to create invalid paths. For example, the following \n  command that searches for duplicate files in the current directory in Bash, will likely fail in the default\n  Windows shell:\n  \n      fclones group *\n      \n  If you need path globbing, and your shell does not support it,\n  use the builtin path globbing provided by `--name` or `--path`.     \n                          \n## The Algorithm\nFiles are processed in several stages. Each stage except the last one is parallel, but \nthe previous stage must complete fully before the next one is started.\n1. Scan input files and filter files matching the selection criteria. Walk directories recursively if requested. \n   Follow symbolic links if requested. For files that match the selection criteria, read their size.\n2. Group collected files by size by storing them in a hash-map. Remove groups smaller than the desired lower-bound \n   (default 2). \n3. In each group, remove duplicate files with the same inode id. The same file could be reached through different\n   paths when hardlinks are present. This step can be optionally skipped.\n4. For each remaining file, compute a hash of a tiny block of initial data. Put files with different hashes \n   into separate groups. Prune result groups if needed. \n5. For each remaining file, compute a hash of a tiny block of data at the end of the file. \n   Put files with different hashes into separate groups. Prune small groups if needed.\n6. For each remaining file, compute a hash of the whole contents of the file. Note that for small files\n   we might have already computed a full contents hash in step 4, therefore these files can be safely\n   omitted. Same as in steps 4 and 5, split groups and remove the ones that are too small.\n7. Write report to the stdout.          \n    \nNote that there is no byte-by-byte comparison of files anywhere. All available hash functions are at least \n128-bit wide, and you don't need to worry about hash collisions. At 10<sup>15</sup> files, \nthe probability of collision is 0.000000001 when using a 128-bit hash, \nwithout taking into account the requirement for the files to also match by size.\n\n### Hashes\nYou can select the hash function with `--hash-fn` (default: `metro`).\nNon-cryptographic hashes are much more efficient than cryptographic, \nhowever you probably won't see much difference unless you're reading from a fast SSD or if file data is cached.\n\n| Hash function                                               | Hash width | Cryptographic | \n|-------------------------------------------------------------|------------|---------------|\n| [metro](http://www.jandrewrogers.com/2015/05/27/metrohash/) | 128-bit    | No            |\n| [xxhash3](https://cyan4973.github.io/xxHash/)               | 128-bit    | No            |\n| [blake3](https://github.com/BLAKE3-team/BLAKE3)             | 256-bit    | Yes           |         \n| [sha256](https://en.wikipedia.org/wiki/SHA-2)               | 256-bit    | Yes           | \n| [sha512](https://en.wikipedia.org/wiki/SHA-2)               | 512-bit    | Yes           | \n| [sha3-256](https://en.wikipedia.org/wiki/SHA-3)             | 256-bit    | Yes           |\n| [sha3-512](https://en.wikipedia.org/wiki/SHA-3)             | 512-bit    | Yes           |\n\n## Tuning\nThis section provides hints on getting the best performance from `fclones`.\n\n### Incremental Mode\nIf you expect to run `fclones group` more than once on the same set of files, \nyou might benefit from turning on the hash cache by adding the `--cache` flag:\n\n```\nfclones group --cache <dir>\n```\n\nCaching can dramatically improve grouping speed on subsequent runs of `fclones` at the expense of some additional\nstorage space needed for the cache. Caching also allows for resuming work quickly after interruption, so it is\nrecommended if you plan to run `fclones` on huge data sets.\n\nThe cache works as follows:\n- Each newly computed file hash is persisted in the cache together with some metadata of the file such as \n  its modification timestamp and length.\n- Whenever a file hash needs to be computed, it is first looked up in the cache. \n  The cached hash is used if the current metadata of the file strictly matches the metadata stored in the cache.  \n\nCached hashes are not invalidated by file moves because files are identified \nby their internal identifiers (inode identifiers on Unix), not by path names, and moves/renames typically preserve \nthose.   \n\nBeware that caching relies on file metadata to detect changes in file contents.\nThis might introduce some inaccuracies to the grouping process if a file modification timestamp and file length\nis not updated immediately whenever a file gets modified. \nMost file systems update the timestamps automatically on closing the file. Therefore, changed files that are held \nopen for a long time (e.g. by database systems) might be not noticed by `fclones group` and might use stale \ncached values.\n\nThe cache database is located in the standard cache directory of the user account. Typically, those are: \n* Linux: `$HOME/.cache/fclones`\n* macOS: `$HOME/Library/Caches/fclones`\n* Windows: `$HOME/AppData/Local/fclones`\n\n### Configuring Parallelism\nThe `--threads` parameter controls the sizes of the internal thread-pool(s). \nThis can be used to reduce parallelism level when you don't want `fclones` to \nimpact performance of your system too much, e.g. when you need to do some other work\nat the same time. We recommended reducing the parallelism level if you need\nto reduce memory usage. \n\nWhen using `fclones` up to version 0.6.x to deduplicate files of sizes of at least a few MBs each  \non spinning drives (HDD), it is recommended to set `--threads 1`, because accessing big files \nfrom multiple threads on HDD can be much slower than single-threaded access \n(YMMV, this is heavily OS-dependent, 2x-10x performance differences have been reported).\n \nSince version 0.7.0, fclones uses separate per-device thread-pools for final hashing \nand it will automatically tune the level of parallelism, memory buffer sizes and partial hashing sizes \nbased on the device type. These automatic settings can be overridden with `-threads` as well.\n\nThe following options can be passed to `--threads`. The more specific options override the less specific ones.\n- `main:<n>` – sets the size of the main thread-pool used for random I/O: directory tree scanning, \n   file metadata fetching and in-memory sorting/hashing.\n   These operations typically benefit from high parallelism level, even on spinning drives. \n   Unset by default, which means the pool will be configured to use all available CPU cores.\n- `dev:<device>:<r>,<s>` – sets the size of the thread-pool `r` used for random I/O and `s` used for \n   sequential I/O on the block device with the given name. The name of the device is OS-dependent. \n   Note this is not the same as the partition name or mount point.\n- `ssd:<r>,<s>` – sets the sizes of the thread-pools used for I/O on solid-state drives. Unset by default. \n- `hdd:<r>,<s>` – sets the sizes of the thread-pools used for I/O on spinning drives. \n   Defaults to `8,1`\n- `removable:<r>,<s>` –  sets the size of the thread-pools used for I/O \n   on removable devices (e.g. USB sticks). Defaults to `4,1`\n- `unknown:<r>,<s>` –  sets the size of the thread-pools used for I/O on devices of unknown type.\n   Sometimes the device type can't be determined e.g. if it is mounted as NAS.\n   Defaults to `4,1`\n- `default:<r>,<s>` – sets the pool sizes to be used by all unset options\n- `<r>,<s>` - same as `default:<r>,<s>`  \n- `<n>` - same as `default:<n>,<n>`\n\n### Examples\nTo limit the parallelism level for the main thread pool to 1:\n\n    fclones group <paths> --threads main:1  \n  \nTo limit the parallelism level for all I/O access for all SSD devices:\n\n    fclones group <paths> --threads ssd:1 \n\nTo set the parallelism level to the number of cores for random I/O access and to \n2 for sequential I/O access for `/dev/sda` block device:\n\n    fclones group <paths> --threads dev:/dev/sda:0,2 \n    \nMultiple `--threads` options can be given, separated by spaces:\n\n    fclones group <paths> --threads main:16 ssd:4 hdd:1,1     \n    \n    \n## Benchmarks\nDifferent duplicate finders were given a task to find duplicates in a large set of files. \nBefore each run, the system page cache was evicted with `echo 3 > /proc/sys/vm/drop_caches`.\n\n### SSD Benchmark\n- Model: Dell Precision 5520\n- CPU: Intel(R) Xeon(R) CPU E3-1505M v6 @ 3.00GHz\n- RAM: 32 GB\n- Storage: local NVMe SSD 512 GB \n- System: Ubuntu Linux 20.10, kernel 5.8.0-53-generic\n- Task: 1,460,720 paths, 316 GB of data       \n\nProgram                                                |  Version  | Language  | Time              | Peak Memory          \n-------------------------------------------------------|-----------|-----------|------------------:|--------------\nfclones                                                |  0.12.1   | Rust      |  0:34.59          | 266 MB\n[yadf](https://github.com/jRimbault/yadf)              |  0.15.2   | Rust      |  0:59.32          | 329 MB\n[czkawka](https://qarmin.github.io/czkawka/)           |  3.1.0    | Rust      |  2:09.00          | 1.4 GB\n[rmlint](https://github.com/sahib/rmlint)              |  2.9.0    | C, Python |  2:28.43          | 942 MB\n[jdupes](https://github.com/jbruchon/jdupes)           |  1.18.2   | C         |  5:01.91          | 332 MB\n[dupe-krill](https://github.com/kornelski/dupe-krill)  |  1.4.5    | Rust      |  5:09.52          | 706 MB\n[fdupes](https://github.com/adrianlopezroche/fdupes)   |  2.1.1    | C         |  5:46.19          | 342 MB\n[rdfind](https://github.com/pauldreik/rdfind)          |  1.4.1    | C++       |  5:53.07          | 496 MB\n[dupeguru](https://dupeguru.voltaicideas.net/)         |  4.1.1    | Python    |  7:49.89          | 1.4 GB\n[fdupes-java](https://github.com/cbismuth/fdupes-java) |  1.3.1    | Java      |  &gt; 20 minutes  | 4.2 GB    \n\n\n`fdupes-java` did not finish the test. I interrupted it after 20 minutes while\nit was still computing MD5 in stage 2/3. Unfortunately `fdupes-java` doesn't display\na useful progress bar, so it is not possible to estimate how long it would take.\n\n### HDD Benchmark \n- Model: Dell Precision M4600\n- CPU: Intel(R) Core(TM) i7-2760QM CPU @ 2.40GHz\n- RAM: 24 GB\n- System: Mint Linux 19.3, kernel 5.4.0-70-generic\n- Storage: Seagate Momentus 7200 RPM SATA drive, EXT4 filesystem  \n- Task: 51370 paths, 2 GB data, 6811 (471 MB) duplicate files\n\nCommands used:\n\n      /usr/bin/time -v fclones -R <file set root> \n      /usr/bin/time -v jdupes -R -Q <file set root>\n      /usr/bin/time -v fdupes -R <file set root>\n      /usr/bin/time -v rdfind <file set root>\n\nIn this benchmark, the page cache was dropped before each run.\n            \nProgram                                                |  Version  | Language | Threads |  Time           |  Peak Memory\n-------------------------------------------------------|-----------|----------|--------:|----------------:|-------------:\nfclones                                                |  0.9.1    | Rust     | 1       |   0:19.45       |  18.1 MB\n[rdfind](https://github.com/pauldreik/rdfind)          |  1.3.5    | C++      | 1       |   0:33.70       |  18.5 MB\n[yadf](https://github.com/jRimbault/yadf)              |  0.14.1   | Rust     |         |   1:11.69       |  22.9 MB\n[jdupes](https://github.com/jbruchon/jdupes)           |  1.9      | C        | 1       |   1:18.47       |  15.7 MB\n[fdupes](https://github.com/adrianlopezroche/fdupes)   |  1.6.1    | C        | 1       |   1:33.71       |  15.9 MB"
}
